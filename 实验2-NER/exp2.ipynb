{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 实验二：命名实体识别\n",
    "\n",
    "**实验任务：**\n",
    "1. 阅读、理解并运行本次实验提供的利用HMM和CRF进行命名实体识别的代码。 （1分）\n",
    "1. 实现一个命名实体识别的评价程序，该程序可以在实体级别计算测试结果的每种实体以及总体的Precision、Recall和F1值。（3分）\n",
    "1. 根据给定的提示，在本Notebook中指定位置，实现一个基于最大熵模型的实体识别系统，并利用ner_char_data目录下的train.txt文件训练模型，利用test.txt文件测试模型效果。（3分）\n",
    "1. 在本Notebook中指定位置，利用给定的ner_clue_data目录下的训练数据文件train.txt分别训练HMM、ME和CRF模型,并使用dev.txt文件里的数据来测试三个模型。输出每个模型在dev.txt数据上的测试结果（每个模型对应一个结果文件），在Notebook中输出每个模型对应的每种实体以及总体的Precision、Recall和F1值。（6分）\n",
    "1. 在本Notebook中指定位置，基于给定的参考项目，利用ner_clue_data目录下的训练数据文件train.txt训练一个BiLSTM+CRF模型,并使用dev.txt文件里的数据来测试模型。输出模型在dev.txt数据上的测试结果，在Notebook中输出模型对应的每种实体以及总体的Precision、Recall和F1值。（2分）\n",
    "1. 按照课程QQ群里给定的模板完成本次实验的实验报告，并按照要求提交。 （5分）\n",
    "\n",
    "**实验提交截止时间：**\n",
    "* 2022年12月10日 晚上10点\n",
    "\n",
    "**实验提交方式：**\n",
    "* 在百度AI Studio的课程中提交，实验报告作为附件上传。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from score import SeqEntityScore\r\n",
    "from tqdm import tqdm\r\n",
    "import numpy as np\r\n",
    "from score import SeqEntityScore\r\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  1. 数据集介绍和数据读取练习\n",
    "## 1.1 对 ner_char_data目录下的数据进行读取\n",
    "\n",
    "ner_char_data 目录下数据为预处理后的数据，包括训练数据文件train.txt和测试数据文件test.txt。数据文件中的每一行由两部分组成字符和字符对应的实体标记，两部分之间用空格分隔。实体标记采用BMES的方式。文件中不同的句子之间用空行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 读取并处理 ner_char_data 目录下的数据文件 \"\"\"\r\n",
    "def data_build(file_name: str, make_vocab=True):\r\n",
    "    word_lists = []\r\n",
    "    tag_lists = []\r\n",
    "    with open('./ner_char_data/' + file_name, 'r', encoding='utf-8') as file_read:\r\n",
    "        word_list = []\r\n",
    "        tag_list = []\r\n",
    "        for line in file_read:\r\n",
    "            if line != '\\n':\r\n",
    "                word, tag = line.strip('\\n').split()\r\n",
    "                word_list.append(word)\r\n",
    "                tag_list.append(tag)\r\n",
    "            else:\r\n",
    "                word_lists.append(word_list)\r\n",
    "                tag_lists.append(tag_list)\r\n",
    "                word_list = []\r\n",
    "                tag_list = []\r\n",
    "    \r\n",
    "    if make_vocab == True:\r\n",
    "        word2id = {}\r\n",
    "        for word_list in word_lists:\r\n",
    "            for word in word_list:\r\n",
    "                if word not in word2id:\r\n",
    "                    word2id[word] = len(word2id)\r\n",
    "        tag2id = {}\r\n",
    "        for tag_list in tag_lists:\r\n",
    "            for tag in tag_list:\r\n",
    "                if tag not in tag2id:\r\n",
    "                    tag2id[tag] = len(tag2id)\r\n",
    "        return word_lists, tag_lists, word2id, tag2id\r\n",
    "    return word_lists, tag_lists\r\n",
    "\r\n",
    "train_word_lists, train_tag_lists, word2id, tag2id = data_build(file_name=\"train.txt\", make_vocab=True)\r\n",
    "id2tag = dict([items[1], items[0]] for items in tag2id.items())   \r\n",
    "test_word_lists, test_tag_lists = data_build(file_name=\"test.txt\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-NAME': 0,\n",
       " 'E-NAME': 1,\n",
       " 'O': 2,\n",
       " 'B-CONT': 3,\n",
       " 'M-CONT': 4,\n",
       " 'E-CONT': 5,\n",
       " 'B-RACE': 6,\n",
       " 'E-RACE': 7,\n",
       " 'B-TITLE': 8,\n",
       " 'M-TITLE': 9,\n",
       " 'E-TITLE': 10,\n",
       " 'B-EDU': 11,\n",
       " 'M-EDU': 12,\n",
       " 'E-EDU': 13,\n",
       " 'B-ORG': 14,\n",
       " 'M-ORG': 15,\n",
       " 'E-ORG': 16,\n",
       " 'M-NAME': 17,\n",
       " 'B-PRO': 18,\n",
       " 'M-PRO': 19,\n",
       " 'E-PRO': 20,\n",
       " 'S-RACE': 21,\n",
       " 'S-NAME': 22,\n",
       " 'B-LOC': 23,\n",
       " 'M-LOC': 24,\n",
       " 'E-LOC': 25,\n",
       " 'M-RACE': 26,\n",
       " 'S-ORG': 27}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  1.2 对ner_clue_data目录下的数据进行读取\n",
    "\n",
    "### 实体类别：\n",
    "```\n",
    "包含10个标签，分别为: 地址（address），书名（book），公司（company），游戏（game），政府（government），电影（movie），姓名（name），组织机构（organization），职位（position），景点（scene）\n",
    "```\n",
    "### 标签类别定义 & 标注规则：\n",
    "```\n",
    "地址（address）: **省**市**区**街**号，**路，**街道，**村等（如单独出现也标记）。地址是标记尽量完全的, 标记到最细。\n",
    "书名（book）: 小说，杂志，习题集，教科书，教辅，地图册，食谱，书店里能买到的一类书籍，包含电子书。\n",
    "公司（company）: **公司，**集团，**银行（央行，中国人民银行除外，二者属于政府机构）, 如：新东方，包含新华网/中国军网等。\n",
    "游戏（game）: 常见的游戏，注意有一些从小说，电视剧改编的游戏，要分析具体场景到底是不是游戏。\n",
    "政府（government）: 包括中央行政机关和地方行政机关两级。 中央行政机关有国务院、国务院组成部门（包括各部、委员会、中国人民银行和审计署）、国务院直属机构（如海关、税务、工商、环保总局等），军队等。\n",
    "电影（movie）: 电影，也包括拍的一些在电影院上映的纪录片，如果是根据书名改编成电影，要根据场景上下文着重区分下是电影名字还是书名。\n",
    "姓名（name）: 一般指人名，也包括小说里面的人物，宋江，武松，郭靖，小说里面的人物绰号：及时雨，花和尚，著名人物的别称，通过这个别称能对应到某个具体人物。\n",
    "组织机构（organization）: 篮球队，足球队，乐团，社团等，另外包含小说里面的帮派如：少林寺，丐帮，铁掌帮，武当，峨眉等。\n",
    "职位（position）: 古时候的职称：巡抚，知州，国师等。现代的总经理，记者，总裁，艺术家，收藏家等。\n",
    "景点（scene）: 常见旅游景点如：长沙公园，深圳动物园，海洋馆，植物园，黄河，长江等。\n",
    "```\n",
    "\n",
    "### 数据来源：\n",
    "[数据下载](https://github.com/CLUEbenchmark/CLUENER2020)\n",
    "\n",
    "```\n",
    "本数据是在清华大学开源的文本分类数据集THUCTC基础上，选出部分数据进行细粒度命名实体标注，原数据来源于Sina News RSS.\n",
    "```\n",
    "\n",
    "### 数据分布：\n",
    "```\n",
    "训练集(train.txt)：10748\n",
    "验证集集(dev.txt)：1343\n",
    "\n",
    "按照不同标签类别统计，训练集数据分布如下（注：一条数据中出现的所有实体都进行标注，如果一条数据出现两个地址（address）实体，那么统计地址（address）类别数据的时候，算两条数据）：\n",
    "【训练集】标签数据分布如下：\n",
    "地址（address）:2829\n",
    "书名（book）:1131\n",
    "公司（company）:2897\n",
    "游戏（game）:2325\n",
    "政府（government）:1797\n",
    "电影（movie）:1109\n",
    "姓名（name）:3661\n",
    "组织机构（organization）:3075\n",
    "职位（position）:3052\n",
    "景点（scene）:1462\n",
    "\n",
    "【验证集】标签数据分布如下：\n",
    "地址（address）:364\n",
    "书名（book）:152\n",
    "公司（company）:366\n",
    "游戏（game）:287\n",
    "政府（government）:244\n",
    "电影（movie）:150\n",
    "姓名（name）:451\n",
    "组织机构（organization）:344\n",
    "职位（position）:425\n",
    "景点（scene）:199\n",
    "```\n",
    "\n",
    "### 数据字段解释：\n",
    "```\n",
    "以train.json为例，数据分为两列：text & label，其中text列代表文本，label列代表文本中出现的所有包含在10个类别中的实体。\n",
    "例如：\n",
    "  text: \"北京勘察设计协会副会长兼秘书长周荫如\"\n",
    "  label: {\"organization\": {\"北京勘察设计协会\": [[0, 7]]}, \"name\": {\"周荫如\": [[15, 17]]}, \"position\": {\"副会长\": [[8, 10]], \"秘书长\": [[12, 14]]}}\n",
    "  其中，organization，name，position代表实体类别，\n",
    "  \"organization\": {\"北京勘察设计协会\": [[0, 7]]}：表示原text中，\"北京勘察设计协会\" 是类别为 \"组织机构（organization）\" 的实体, 并且start_index为0，end_index为7 （注：下标从0开始计数）\n",
    "  \"name\": {\"周荫如\": [[15, 17]]}：表示原text中，\"周荫如\" 是类别为 \"姓名（name）\" 的实体, 并且start_index为15，end_index为17\n",
    "  \"position\": {\"副会长\": [[8, 10]], \"秘书长\": [[12, 14]]}：表示原text中，\"副会长\" 是类别为 \"职位（position）\" 的实体, 并且start_index为8，end_index为10，同时，\"秘书长\" 也是类别为 \"职位（position）\" 的实体,\n",
    "  并且start_index为12，end_index为14\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在这里练习读取并处理 ner_clue_data目录下的数据。\n",
    "# train_word_lists, train_tag_lists, word2id, tag2id \n",
    "\"\"\" 读取并处理 ner_char_data 目录下的数据文件 \"\"\"\n",
    "import json\n",
    "    \n",
    "def data_build_gluener(file_name:str, make_vocab=True):\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open('./ner_clue_data/' + file_name, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        for line in f:\n",
    "            json_data = {}\n",
    "            line = json.loads(line.strip())\n",
    "            text = line['text']\n",
    "            label_items = line.get('label', None)\n",
    "            \n",
    "            # 标注\n",
    "            labels = ['O']*len(text)\n",
    "            if(label_items != None):\n",
    "                for key,value in label_items.items():\n",
    "                    for name, index in value.items():\n",
    "                        for start_idx, end_idx in index:\n",
    "                            assert text[start_idx:end_idx + 1] == name\n",
    "                            # if(len(name) == 1):\n",
    "                            #     print(name)\n",
    "                            if(start_idx == end_idx):\n",
    "                                labels[start_idx] = 'S-' + key\n",
    "                            else:\n",
    "                                labels[start_idx] = 'B-'+ key\n",
    "                                labels[start_idx+1: end_idx+1] = ['I-'+key]*(end_idx - start_idx)\n",
    "            word_lists.append(list(text))\n",
    "            tag_lists.append(labels)\n",
    "\n",
    "    if make_vocab == True:\n",
    "        word2id = {}\n",
    "        for word_list in word_lists:\n",
    "            for word in word_list:\n",
    "                if word not in word2id:\n",
    "                    word2id[word] = len(word2id)\n",
    "        tag2id = {}\n",
    "        for tag_list in tag_lists:\n",
    "            for tag in tag_list:\n",
    "                if tag not in tag2id:\n",
    "                    tag2id[tag] = len(tag2id)\n",
    "        return word_lists, tag_lists, word2id, tag2id\n",
    "\n",
    "    return word_lists, tag_lists\n",
    "             \n",
    "\n",
    "clue_train_word_lists, clue_train_tag_lists, clue_word2id, clue_tag2id = data_build_gluener(file_name=\"train.txt\", make_vocab=True)\n",
    "clue_dev_word_lists, clue_dev_tag_lists = data_build_gluener(file_name=\"dev.txt\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-company': 0,\n",
       " 'I-company': 1,\n",
       " 'O': 2,\n",
       " 'B-name': 3,\n",
       " 'I-name': 4,\n",
       " 'B-game': 5,\n",
       " 'I-game': 6,\n",
       " 'B-organization': 7,\n",
       " 'I-organization': 8,\n",
       " 'B-movie': 9,\n",
       " 'I-movie': 10,\n",
       " 'B-position': 11,\n",
       " 'I-position': 12,\n",
       " 'B-address': 13,\n",
       " 'I-address': 14,\n",
       " 'B-government': 15,\n",
       " 'I-government': 16,\n",
       " 'B-scene': 17,\n",
       " 'I-scene': 18,\n",
       " 'B-book': 19,\n",
       " 'I-book': 20,\n",
       " 'S-company': 21,\n",
       " 'S-address': 22,\n",
       " 'S-name': 23,\n",
       " 'S-position': 24}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clue_tag2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. 隐马尔科夫（HMM）模型\n",
    "## 2.1隐马尔科夫模型参数的计算\n",
    "利用训练数据获取HMM模型对应的参数**A**，**B**和**Pi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" HMM 参数构建 \"\"\"\r\n",
    "import numpy as np\r\n",
    "# N: 状态数，这里对应存在的标注的种类 \r\n",
    "# M: 观测数，这里对应有多少不同的字\r\n",
    "N, M = len(tag2id), len(word2id)\r\n",
    "# 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率\r\n",
    "A = np.zeros(shape=(N, N), dtype=float)\r\n",
    "# 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率\r\n",
    "B = np.zeros(shape=(N, M), dtype=float)\r\n",
    "# 初始状态概率  Pi[i]表示初始时刻为状态i的概率\r\n",
    "Pi = np.zeros(shape=N, dtype=float)\r\n",
    "\r\n",
    "\"\"\" 构建转移概率矩阵 \"\"\"\r\n",
    "for tag_list in train_tag_lists:\r\n",
    "    seq_len = len(tag_list)\r\n",
    "    for i in range(seq_len - 1):\r\n",
    "        current_tagid = tag2id[tag_list[i]]\r\n",
    "        next_tagid = tag2id[tag_list[i+1]]\r\n",
    "        A[current_tagid][next_tagid] += 1\r\n",
    "A[A == 0.] = 1e-10  # 平滑处理\r\n",
    "A = A / np.sum(a=A, axis=1, keepdims=True)\r\n",
    "\r\n",
    "\"\"\" 构建观测概率矩阵 \"\"\"\r\n",
    "for tag_list, word_list in zip(train_tag_lists, train_word_lists):\r\n",
    "    assert len(tag_list) == len(word_list)\r\n",
    "    for tag, word in zip(tag_list, word_list):\r\n",
    "        tag_id = tag2id[tag]\r\n",
    "        word_id = word2id[word]\r\n",
    "        B[tag_id][word_id] += 1\r\n",
    "B[B == 0.] = 1e-10  # 平滑处理\r\n",
    "B = B / np.sum(a=B, axis=1, keepdims=True)\r\n",
    "\r\n",
    "\"\"\" 构建初始状态概率 \"\"\"\r\n",
    "for tag_list in train_tag_lists:\r\n",
    "    init_tagid = tag2id[tag_list[0]]\r\n",
    "    Pi[init_tagid] += 1\r\n",
    "Pi[Pi == 0.] = 1e-10  # 平滑处理\r\n",
    "Pi = Pi / np.sum(a=Pi)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 维特比算法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 维特比算法 \"\"\"\r\n",
    "def viterbi(word_list, word2id, tag2id):\r\n",
    "    \"\"\"\r\n",
    "    使用维特比算法对给定观测序列求状态序列， 这里就是对字组成的序列,求其对应的标注。\r\n",
    "    维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）\r\n",
    "    这时一条路径对应着一个状态序列\r\n",
    "    \"\"\"\r\n",
    "    # 问题:整条链很长的情况下，十分多的小概率相乘，最后可能造成下溢\r\n",
    "    # 解决办法：采用对数概率，这样源空间中的很小概率，就被映射到对数空间的大的负数\r\n",
    "    #  同时相乘操作也变成简单的相加操作\r\n",
    "    ALog = np.log(A)\r\n",
    "    BLog = np.log(B)\r\n",
    "    PiLog = np.log(Pi)\r\n",
    "\r\n",
    "    # 初始化 维比特矩阵viterbi 它的维度为[状态数, 序列长度]\r\n",
    "    # 其中viterbi[i, j]表示标注序列的第j个标注为i的所有单个序列(i_1, i_2, ..i_j)出现的概率最大值\r\n",
    "    seq_len = len(word_list)\r\n",
    "    viterbi = np.zeros(shape=(N, seq_len), dtype=float)\r\n",
    "    # backpointer是跟viterbi一样大小的矩阵\r\n",
    "    # backpointer[i, j]存储的是 标注序列的第j个标注为i时，第j-1个标注的id\r\n",
    "    # 等解码的时候，我们用backpointer进行回溯，以求出最优路径\r\n",
    "    backpointer = np.zeros(shape=(N, seq_len), dtype=float)\r\n",
    "\r\n",
    "    # Pi[i] 表示第一个字的标记为i的概率\r\n",
    "    # Bt[word_id]表示字为word_id的时候，对应各个标记的概率\r\n",
    "    # A.t()[tag_id]表示各个状态转移到tag_id对应的概率\r\n",
    "\r\n",
    "    # 所以第一步为\r\n",
    "    start_wordid = word2id.get(word_list[0], None)\r\n",
    "    Bt = BLog.T\r\n",
    "    if start_wordid is None:\r\n",
    "        # 如果字不再字典里，则假设状态的概率分布是均匀的\r\n",
    "        bt = np.log(np.ones(shape=N, dtype=float) / N)\r\n",
    "    else:\r\n",
    "        bt = Bt[start_wordid]\r\n",
    "    viterbi[:, 0] = PiLog + bt\r\n",
    "    backpointer[:, 0] = -1\r\n",
    "\r\n",
    "    # 递推公式：viterbi[tag_id, step] = max(viterbi[:, step-1]* A.t()[tag_id] * Bt[word])\r\n",
    "    # 其中word是step时刻对应的字, 由上述递推公式求后续各步\r\n",
    "    for step in range(1, seq_len):\r\n",
    "        wordid = word2id.get(word_list[step], None)\r\n",
    "        # 处理字不在字典中的情况\r\n",
    "        # bt是在t时刻字为wordid时，状态的概率分布\r\n",
    "        if wordid is None:\r\n",
    "            # 如果字不再字典里，则假设状态的概率分布是均匀的\r\n",
    "            bt = np.log(np.ones(N) / N)\r\n",
    "        else:\r\n",
    "            bt = Bt[wordid]  # 否则从观测概率矩阵中取bt\r\n",
    "        for tag_id in range(len(tag2id)):\r\n",
    "            # (step-1)的状态自带的概率viterbi[:, step - 1]  ；(step-1)的状态 转移到tag_id的转移概率ALog[:, tag_id]\r\n",
    "            max_prob = np.max(a=viterbi[:, step - 1] + ALog[:, tag_id], axis=0) \r\n",
    "            max_id = np.argmax(a=viterbi[:, step - 1] + ALog[:, tag_id], axis=0) \r\n",
    "            viterbi[tag_id, step] = max_prob + bt[tag_id]\r\n",
    "            backpointer[tag_id, step] = max_id\r\n",
    "\r\n",
    "\r\n",
    "    # for step in range(1, seq_len):\r\n",
    "    #     wordid = wordid.get(word_list[step], None)\r\n",
    "    #     for tag_id in range(len(tag2id)):\r\n",
    "    #         max_id = np.argmax(viterbi[:, step - 1] + ALog[:tag_id], axis = 0)\r\n",
    "    #         max_prob = np.max(viterbi[:,step-1] + A[:,tag_id], axis = 0)\r\n",
    "    #         viterbi[tag_id, step] = max_prob + BLog[tag_id,wordid]\r\n",
    "    #         backpointer [tag_id, step] = max_id\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    # 终止， t=seq_len 即 viterbi[:, seq_len]中的最大概率，就是最优路径的概率\r\n",
    "    best_path_prob = np.max(a=viterbi[:, seq_len - 1], axis=0)\r\n",
    "    best_path_pointer = np.argmax(a=viterbi[:, seq_len - 1], axis=0)\r\n",
    "\r\n",
    "    # 回溯，求最优路径\r\n",
    "    best_path_pointer = int(best_path_pointer)\r\n",
    "    best_path = [best_path_pointer]\r\n",
    "\r\n",
    "    for back_step in range(seq_len-1, 0, -1):\r\n",
    "        best_path_pointer = backpointer[best_path_pointer, back_step]\r\n",
    "        best_path_pointer = int(best_path_pointer)\r\n",
    "        best_path.append(best_path_pointer)\r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "    # 将tag_id组成的序列转化为tag\r\n",
    "    assert len(best_path) == len(word_list)\r\n",
    "    id2tag = dict((id_, tag) for tag, id_ in tag2id.items())\r\n",
    "    tag_list = [id2tag[id_] for id_ in reversed(best_path)]\r\n",
    "\r\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 利用HMM模型和viterbi进行实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 利用HMM识别ner_char_data目录下test.txt中的数据\"\"\"\r\n",
    "pred_tag_lists = []\r\n",
    "for word_list in test_word_lists:\r\n",
    "    pred_tag_list = viterbi(word_list, word2id, tag2id)\r\n",
    "    pred_tag_lists.append(pred_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4 按标记对HMM的识别结果进行评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "         B-NAME     0.9800    0.8750    0.9245       112\n",
      "         M-NAME     0.9459    0.8537    0.8974        82\n",
      "         E-NAME     0.9000    0.8036    0.8491       112\n",
      "              O     0.9568    0.9177    0.9369      5190\n",
      "          B-PRO     0.5581    0.7273    0.6316        33\n",
      "          E-PRO     0.6512    0.8485    0.7368        33\n",
      "          B-EDU     0.9000    0.9643    0.9310       112\n",
      "          E-EDU     0.9167    0.9821    0.9483       112\n",
      "        B-TITLE     0.8811    0.8925    0.8867       772\n",
      "        M-TITLE     0.9038    0.8751    0.8892      1922\n",
      "        E-TITLE     0.9514    0.9637    0.9575       772\n",
      "          B-ORG     0.8422    0.8879    0.8644       553\n",
      "          M-ORG     0.9002    0.9327    0.9162      4325\n",
      "          E-ORG     0.8262    0.8680    0.8466       553\n",
      "         B-CONT     0.9655    1.0000    0.9825        28\n",
      "         M-CONT     0.9815    1.0000    0.9907        53\n",
      "         E-CONT     0.9655    1.0000    0.9825        28\n",
      "          M-EDU     0.9348    0.9609    0.9477       179\n",
      "         B-RACE     1.0000    0.9286    0.9630        14\n",
      "         E-RACE     1.0000    0.9286    0.9630        14\n",
      "          B-LOC     0.3333    0.3333    0.3333         6\n",
      "          M-LOC     0.5833    0.3333    0.4242        21\n",
      "          E-LOC     0.5000    0.5000    0.5000         6\n",
      "          M-PRO     0.4490    0.6471    0.5301        68\n",
      "      avg/total     0.9149    0.9122    0.9130     15100\n",
      "\n",
      "Confusion Matrix:\n",
      "         B-NAME  M-NAME  E-NAME       O   B-PRO   E-PRO   B-EDU   E-EDU B-TITLE M-TITLE E-TITLE   B-ORG   M-ORG   E-ORG  B-CONT  M-CONT  E-CONT   M-EDU  B-RACE  E-RACE   B-LOC   M-LOC   E-LOC   M-PRO \n",
      " B-NAME      98       0       0       8       0       0       0       0       0       0       0       1       2       0       0       0       0       0       0       0       0       0       0       0 \n",
      " M-NAME       0      70       6       3       0       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       0       0       0 \n",
      " E-NAME       0       2      90      16       0       0       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       0       0 \n",
      "      O       0       0       2    4763       3       4       1       2      26      78      26      37     204      30       0       0       0       1       0       0       0       0       0      12 \n",
      "  B-PRO       0       0       0       0      24       0       1       0       0       0       0       0       5       0       0       0       0       0       0       0       0       0       0       3 \n",
      "  E-PRO       0       0       0       0       0      28       1       1       0       0       0       0       0       2       0       0       0       1       0       0       0       0       0       0 \n",
      "  B-EDU       0       0       0       0       0       0     108       0       0       0       0       0       1       0       0       0       0       3       0       0       0       0       0       0 \n",
      "  E-EDU       0       0       0       0       0       1       0     110       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0 \n",
      "B-TITLE       0       0       0      20       2       0       2       0     689      28       1       6      23       1       0       0       0       0       0       0       0       0       0       0 \n",
      "M-TITLE       0       0       0      44       1       3       2       3      35    1682       6       3     115      17       0       0       0       4       0       0       0       0       0       7 \n",
      "E-TITLE       0       0       0       6       0       0       0       1       0       2     744       4      15       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-ORG       1       0       0      28       0       0       0       0       6       0       0     491      23       0       1       0       0       0       0       0       3       0       0       0 \n",
      "  M-ORG       1       2       2      70      10       7       3       3      17      53       4      38    4034      42       0       1       1       1       0       0       0       5       3      25 \n",
      "  E-ORG       0       0       0      10       1       0       1       0       9      18       1       0      30     480       0       0       0       0       0       0       0       0       0       3 \n",
      " B-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0 \n",
      " M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0       0       0       0       0 \n",
      " E-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0 \n",
      "  M-EDU       0       0       0       1       1       0       0       0       0       0       0       0       0       1       0       0       0     172       0       0       0       0       0       4 \n",
      " B-RACE       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0       0 \n",
      " E-RACE       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0 \n",
      "  B-LOC       0       0       0       1       0       0       0       0       0       0       0       3       0       0       0       0       0       0       0       0       2       0       0       0 \n",
      "  M-LOC       0       0       0       4       0       0       0       0       0       0       0       0       7       2       0       0       0       0       0       0       1       7       0       0 \n",
      "  E-LOC       0       0       0       2       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0       0       0       0       3       0 \n",
      "  M-PRO       0       0       0       0       1       0       1       0       0       0       0       0      18       3       0       0       0       1       0       0       0       0       0      44 \n"
     ]
    }
   ],
   "source": [
    "\"\"\" HMM 评测 \"\"\"\r\n",
    "from evaluating import Metrics\r\n",
    "metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "metrics.report_scores()\r\n",
    "metrics.report_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "           NAME     0.9800    0.8750    0.9245       112\n",
      "            PRO     0.5581    0.7273    0.6316        33\n",
      "            EDU     0.9000    0.9643    0.9310       112\n",
      "          TITLE     0.8811    0.8925    0.8867       772\n",
      "            ORG     0.8422    0.8879    0.8644       553\n",
      "           CONT     0.9655    1.0000    0.9825        28\n",
      "           RACE     1.0000    0.9286    0.9630        14\n",
      "            LOC     0.3333    0.3333    0.3333         6\n",
      "      avg/total     0.8669    0.8914    0.8790      1630\n"
     ]
    }
   ],
   "source": [
    "# 对HMM进行实体级别评测\r\n",
    "from score import SeqEntityScore\r\n",
    "def print_score(id2tag ,gold_tag_lists, pred_tag_lists):\r\n",
    "    metrics = SeqEntityScore(id2tag= id2tag)\r\n",
    "    metrics.update(gold_tag_lists,pred_tag_lists)\r\n",
    "        # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metrics.get_result()\r\n",
    "    metrics.report_scores(result)\r\n",
    "\r\n",
    "id2tag = dict((id_, tag) for tag, id_ in tag2id.items())\r\n",
    "print_score(id2tag,test_tag_lists, pred_tag_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. 条件随机场(CRF)模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 安装sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting sklearn-crfsuite\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn-crfsuite) (4.64.1)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/16/c0/e61ec91560d34518a4986a29898f15248a226e7bf201ade882f5fda8f7c1/python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tabulate in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn-crfsuite) (0.8.3)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn-crfsuite) (1.16.0)\n",
      "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.8 sklearn-crfsuite-0.3.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 从sklearn_crfsuite模块中导入CRF包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/linalg/__init__.py:217: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
      "  from numpy.dual import register_func\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3.3  CRF中的特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\r\n",
    "    \"\"\"抽取单个字的特征\"\"\"\r\n",
    "    word = sent[i]\r\n",
    "    prev_word = \"<s>\" if i == 0 else sent[i-1]\r\n",
    "    next_word = \"</s>\" if i == (len(sent)-1) else sent[i+1]\r\n",
    "    # 使用的特征：\r\n",
    "    # 前一个词，当前词，后一个词，\r\n",
    "    # 前一个词+当前词， 当前词+后一个词\r\n",
    "    features = {\r\n",
    "        'w': word,\r\n",
    "        'w-1': prev_word,\r\n",
    "        'w+1': next_word,\r\n",
    "        'w-1:w': prev_word+word,\r\n",
    "        'w:w+1': word+next_word,\r\n",
    "        'w-1:w:w+1':prev_word+word+next_word,\r\n",
    "        'bias': 1\r\n",
    "    }\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def sent2features(sent):\r\n",
    "    \"\"\"抽取序列特征\"\"\"\r\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3.4  CRF模型的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CRFModel(object):\r\n",
    "    def __init__(self,\r\n",
    "                 algorithm='lbfgs',\r\n",
    "                 c1=0.1,\r\n",
    "                 c2=0.1,\r\n",
    "                 max_iterations=100,\r\n",
    "                 all_possible_transitions=False\r\n",
    "                 ):\r\n",
    "\r\n",
    "        self.model = CRF(algorithm=algorithm,\r\n",
    "                         c1=c1,\r\n",
    "                         c2=c2,\r\n",
    "                         max_iterations=max_iterations,\r\n",
    "                         all_possible_transitions=all_possible_transitions)\r\n",
    "\r\n",
    "    def train(self, sentences, tag_lists):\r\n",
    "        features = [sent2features(s) for s in sentences]\r\n",
    "        self.model.fit(features, tag_lists)\r\n",
    "\r\n",
    "    def test(self, sentences):\r\n",
    "        features = [sent2features(s) for s in sentences]\r\n",
    "        pred_tag_lists = self.model.predict(features)\r\n",
    "        return pred_tag_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3.3  CRF模型训练、测试与评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from evaluating import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "         B-NAME     1.0000    0.9821    0.9910       112\n",
      "         M-NAME     1.0000    0.9756    0.9877        82\n",
      "         E-NAME     1.0000    0.9821    0.9910       112\n",
      "              O     0.9653    0.9659    0.9656      5190\n",
      "          B-PRO     0.9375    0.9091    0.9231        33\n",
      "          E-PRO     0.9375    0.9091    0.9231        33\n",
      "          B-EDU     0.9820    0.9732    0.9776       112\n",
      "          E-EDU     0.9910    0.9821    0.9865       112\n",
      "        B-TITLE     0.9417    0.9417    0.9417       772\n",
      "        M-TITLE     0.9447    0.9069    0.9254      1922\n",
      "        E-TITLE     0.9819    0.9819    0.9819       772\n",
      "          B-ORG     0.9550    0.9584    0.9567       553\n",
      "          M-ORG     0.9436    0.9630    0.9532      4325\n",
      "          E-ORG     0.9189    0.9222    0.9206       553\n",
      "         B-CONT     1.0000    1.0000    1.0000        28\n",
      "         M-CONT     1.0000    1.0000    1.0000        53\n",
      "         E-CONT     1.0000    1.0000    1.0000        28\n",
      "          M-EDU     0.9824    0.9330    0.9570       179\n",
      "         B-RACE     1.0000    1.0000    1.0000        14\n",
      "         E-RACE     1.0000    1.0000    1.0000        14\n",
      "          B-LOC     1.0000    0.8333    0.9091         6\n",
      "          M-LOC     1.0000    0.8095    0.8947        21\n",
      "          E-LOC     1.0000    0.8333    0.9091         6\n",
      "          M-PRO     0.8919    0.9706    0.9296        68\n",
      "      avg/total     0.9552    0.9551    0.9550     15100\n",
      "\n",
      "Confusion Matrix:\n",
      "         B-NAME  M-NAME  E-NAME       O   B-PRO   E-PRO   B-EDU   E-EDU B-TITLE M-TITLE E-TITLE   B-ORG   M-ORG   E-ORG  B-CONT  M-CONT  E-CONT   M-EDU  B-RACE  E-RACE   B-LOC   M-LOC   E-LOC   M-PRO \n",
      " B-NAME     110       0       0       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      " M-NAME       0      80       0       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      " E-NAME       0       0     110       2       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "      O       0       0       0    5013       0       0       0       0       6      12       5      14     125      15       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-PRO       0       0       0       0      30       0       1       0       0       0       0       0       1       0       0       0       0       0       0       0       0       0       0       1 \n",
      "  E-PRO       0       0       0       0       0      30       0       0       0       0       0       0       0       1       0       0       0       1       0       0       0       0       0       1 \n",
      "  B-EDU       0       0       0       0       0       0     109       0       0       0       0       1       1       0       0       0       0       1       0       0       0       0       0       0 \n",
      "  E-EDU       0       0       0       0       0       1       0     110       0       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0 \n",
      "B-TITLE       0       0       0       9       0       0       0       0     727      18       0       6      12       0       0       0       0       0       0       0       0       0       0       0 \n",
      "M-TITLE       0       0       0      55       0       0       1       0      19    1743       6       1      80      16       0       0       0       1       0       0       0       0       0       0 \n",
      "E-TITLE       0       0       0      10       0       0       0       1       0       0     758       0       2       1       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-ORG       0       0       0       9       0       0       0       0      12       0       0     530       2       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  M-ORG       0       0       0      79       1       1       0       0       8      55       2       2    4165      10       0       0       0       0       0       0       0       0       0       2 \n",
      "  E-ORG       0       0       0      11       0       0       0       0       0      17       1       0      14     510       0       0       0       0       0       0       0       0       0       0 \n",
      " B-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0       0       0 \n",
      " M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0       0       0       0       0 \n",
      " E-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      28       0       0       0       0       0       0       0 \n",
      "  M-EDU       0       0       0       1       1       0       0       0       0       0       0       0       5       1       0       0       0     167       0       0       0       0       0       4 \n",
      " B-RACE       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      14       0       0       0       0       0 \n",
      " E-RACE       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      14       0       0       0       0 \n",
      "  B-LOC       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0       0       0       5       0       0       0 \n",
      "  M-LOC       0       0       0       0       0       0       0       0       0       0       0       0       4       0       0       0       0       0       0       0       0      17       0       0 \n",
      "  E-LOC       0       0       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0       0       0       0       0       0       5       0 \n",
      "  M-PRO       0       0       0       0       0       0       0       0       0       0       0       0       2       0       0       0       0       0       0       0       0       0       0      66 \n"
     ]
    }
   ],
   "source": [
    "from evaluating import Metrics\r\n",
    "# 训练CRF模型\r\n",
    "crf_model = CRFModel()\r\n",
    "crf_model.train(train_word_lists, train_tag_lists)\r\n",
    "\r\n",
    "pred_tag_lists = crf_model.test(test_word_lists)\r\n",
    "\r\n",
    "metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "metrics.report_scores()\r\n",
    "metrics.report_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. 实现实体级别评价程序\n",
    "\n",
    "请在下面的Cell中实现一个命名实体识别的评价程序，该程序可以在实体级别计算测试结果中的每种实体以及总体的Precision、Recall和F1值。（3分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from score import SeqEntityScore\n",
    "def print_score(id2tag ,gold_tag_lists, pred_tag_lists):\n",
    "    \"\"\"\n",
    "    @id2tag ：标签id序列到标签序列的转换字典，标签的标注方式应当遵循BIO标注法  \n",
    "    @gold_tag_lists：真实的标签序列构成的列表\n",
    "    @pred_tag_lists: 预测的标签序列构成的列表\n",
    "    \"\"\"\n",
    "    metrics = SeqEntityScore(id2tag= id2tag)\n",
    "    metrics.update(gold_tag_lists,pred_tag_lists)\n",
    "        # 根据metric统计的数据，计算最终的准确率，召回率，F1值\n",
    "    result = metrics.get_result()\n",
    "    metrics.report_scores(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "           NAME     1.0000    0.9821    0.9910       112\n",
      "            PRO     0.9375    0.9091    0.9231        33\n",
      "            EDU     0.9820    0.9732    0.9776       112\n",
      "          TITLE     0.9417    0.9417    0.9417       772\n",
      "            ORG     0.9550    0.9584    0.9567       553\n",
      "           CONT     1.0000    1.0000    1.0000        28\n",
      "           RACE     1.0000    1.0000    1.0000        14\n",
      "            LOC     1.0000    0.8333    0.9091         6\n",
      "      avg/total     0.9545    0.9528    0.9536      1630\n"
     ]
    }
   ],
   "source": [
    "print_score(id2tag,test_tag_lists,pred_tag_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. 基于最大熵模型的实体识别\n",
    "请在下面的Cell中实现一个基于最大熵模型的实体识别系统，并利用ner_char_data目录下的train.txt文件训练模型，利用test.txt文件测试模型效果。（3分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 请在这里实现一个基于最大熵模型的实体识别系统\r\n",
    "# https://blog.csdn.net/gary101818/article/details/121902646\r\n",
    "# 导入sklearn中的LogisticRegression （LogisticRegression即为最大熵模型）\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.feature_extraction import DictVectorizer\r\n",
    "from evaluating import Metrics\r\n",
    "\r\n",
    "# dataloader\r\n",
    "train_tag_lists_flatten = [x for train_tag_list in train_tag_lists for x in train_tag_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\r\n",
    "class MaxEntropyModel(object):\r\n",
    "    def __init__(self):\r\n",
    "        self.model = LogisticRegression(solver=\"saga\", max_iter=100, n_jobs=-1)\r\n",
    "        self.vectorizer = DictVectorizer()\r\n",
    "        \r\n",
    "    def train(self, sentences, tag_lists):\r\n",
    "        features = [feature for s in sentences for feature in sent2features(s) ]\r\n",
    "        features = self.vectorizer.fit_transform(features)\r\n",
    "        self.model.fit(features, tag_lists)\r\n",
    "\r\n",
    "    def test(self, sentences):\r\n",
    "        features = [self.vectorizer.transform(sent2features(s)) for s in sentences]\r\n",
    "        pred_tag_lists = [self.model.predict(feature).tolist() for feature in features]\r\n",
    "        return pred_tag_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "logistic_model = MaxEntropyModel()\n",
    "logistic_model.train(train_word_lists, train_tag_lists_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "     NAME     0.8203    0.9375    0.8750       112\n",
      "      PRO     0.8500    0.5152    0.6415        33\n",
      "      EDU     0.9292    0.9375    0.9333       112\n",
      "    TITLE     0.9119    0.8718    0.8914       772\n",
      "      ORG     0.8893    0.8571    0.8729       553\n",
      "     CONT     0.6000    0.9643    0.7397        28\n",
      "     RACE     1.0000    0.9286    0.9630        14\n",
      "      LOC     0.0000    0.0000    0.0000         6\n",
      "avg/total     0.8882    0.8675    0.8777      1630\n",
      "           precision    recall  f1-score   support\n",
      "   B-NAME     0.8203    0.9375    0.8750       112\n",
      "   M-NAME     0.9762    0.5000    0.6613        82\n",
      "   E-NAME     0.9478    0.9732    0.9604       112\n",
      "        O     0.9606    0.9435    0.9520      5190\n",
      "    B-PRO     0.8500    0.5152    0.6415        33\n",
      "    E-PRO     0.7750    0.9394    0.8493        33\n",
      "    B-EDU     0.9292    0.9375    0.9333       112\n",
      "    E-EDU     0.9908    0.9643    0.9774       112\n",
      "  B-TITLE     0.9119    0.8718    0.8914       772\n",
      "  M-TITLE     0.9069    0.8663    0.8861      1922\n",
      "  E-TITLE     0.9909    0.9870    0.9890       772\n",
      "    B-ORG     0.8893    0.8571    0.8729       553\n",
      "    M-ORG     0.8894    0.9466    0.9171      4325\n",
      "    E-ORG     0.9019    0.8807    0.8911       553\n",
      "   B-CONT     0.6000    0.9643    0.7397        28\n",
      "   M-CONT     1.0000    1.0000    1.0000        53\n",
      "   E-CONT     1.0000    0.9643    0.9818        28\n",
      "    M-EDU     0.9556    0.9609    0.9582       179\n",
      "   B-RACE     1.0000    0.9286    0.9630        14\n",
      "   E-RACE     1.0000    0.9286    0.9630        14\n",
      "    B-LOC     0.0000    0.0000    0.0000         6\n",
      "    M-LOC     0.6000    0.1429    0.2308        21\n",
      "    E-LOC     1.0000    0.8333    0.9091         6\n",
      "    M-PRO     0.5479    0.5882    0.5674        68\n",
      "avg/total     0.9228    0.9219    0.9211     15100\n",
      "\n",
      "Confusion Matrix:\n",
      "         B-NAME  M-NAME  E-NAME       O   B-PRO   E-PRO   B-EDU   E-EDU B-TITLE M-TITLE E-TITLE   B-ORG   M-ORG   E-ORG  B-CONT  M-CONT  E-CONT   M-EDU  B-RACE  E-RACE   B-LOC   M-LOC   E-LOC   M-PRO \n",
      " B-NAME     105       0       0       7       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      " M-NAME       0      41       0       1       0       0       0       0       0       0       0       6      34       0       0       0       0       0       0       0       0       0       0       0 \n",
      " E-NAME       0       0     109       3       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "      O       4       0       2    4897       0       1       0       0      12      35       4      18     201       7       2       0       0       1       0       0       0       0       0       6 \n",
      "  B-PRO       1       0       0       0      17       0       0       0       5       0       0       0      10       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  E-PRO       0       0       0       0       0      31       0       0       0       0       0       0       2       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-EDU       0       0       0       0       0       0     105       0       2       0       0       2       3       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  E-EDU       0       0       0       0       0       1       0     108       0       0       0       0       1       2       0       0       0       0       0       0       0       0       0       0 \n",
      "B-TITLE       1       0       0      10       1       0       5       0     673      39       0      22      19       0       2       0       0       0       0       0       0       0       0       0 \n",
      "M-TITLE       0       1       0      62       0       5       2       0      20    1665       3       2     138      12       0       0       0       4       0       0       0       0       0       8 \n",
      "E-TITLE       0       0       4       5       0       0       0       1       0       0     762       0       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  B-ORG      17       0       0      16       0       0       0       0      11       0       0     474      19       0      14       0       0       0       0       0       2       0       0       0 \n",
      "  M-ORG       0       0       0      85       2       2       1       0      15      72       0       2    4094      31       0       0       0       3       0       0       0       2       0      16 \n",
      "  E-ORG       0       0       0       6       0       0       0       0       0      23       0       0      36     487       0       0       0       0       0       0       0       0       0       1 \n",
      " B-CONT       0       0       0       0       0       0       0       0       0       0       0       1       0       0      27       0       0       0       0       0       0       0       0       0 \n",
      " M-CONT       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      53       0       0       0       0       0       0       0       0 \n",
      " E-CONT       0       0       0       0       0       0       0       0       0       0       0       0       1       0       0       0      27       0       0       0       0       0       0       0 \n",
      "  M-EDU       0       0       0       1       0       0       0       0       0       0       0       0       3       1       0       0       0     172       0       0       0       0       0       2 \n",
      " B-RACE       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0       0 \n",
      " E-RACE       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0      13       0       0       0       0 \n",
      "  B-LOC       0       0       0       0       0       0       0       0       0       0       0       6       0       0       0       0       0       0       0       0       0       0       0       0 \n",
      "  M-LOC       0       0       0       0       0       0       0       0       0       1       0       0      17       0       0       0       0       0       0       0       0       3       0       0 \n",
      "  E-LOC       0       0       0       1       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       5       0 \n",
      "  M-PRO       0       0       0       2       0       0       0       0       0       1       0       0      25       0       0       0       0       0       0       0       0       0       0      40 \n"
     ]
    }
   ],
   "source": [
    "pred_tag_lists = logistic_model.test(test_word_lists)\r\n",
    "# 实体级别\r\n",
    "print_score(id2tag,test_tag_lists, pred_tag_lists)\r\n",
    "# 字符级别\r\n",
    "metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "metrics.report_scores()\r\n",
    "metrics.report_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import score\r\n",
    "import evaluating\r\n",
    "import importlib\r\n",
    "importlib.reload(score)\r\n",
    "importlib.reload(evaluating)\r\n",
    "from score import SeqEntityScore\r\n",
    "from evaluating import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-NAMEM-ORGE-NAMEOOO\n",
      "['B-NAME', 'M-ORG', 'E-NAME', 'O', 'O', 'O']\n",
      "常建良，男，\n",
      "['B-NAME', 'M-ORG', 'E-NAME', 'O', 'O', 'O']\n",
      "常 : NAME\n",
      ">>>>>>>>>>\n",
      "1963年出生，工科学士，高级工程师，北京物资学院客座副教授。\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TITLE', 'M-ORG', 'B-EDU', 'E-EDU', 'O', 'B-TITLE', 'M-TITLE', 'M-TITLE', 'M-TITLE', 'E-TITLE', 'O', 'B-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'E-ORG', 'B-TITLE', 'M-TITLE', 'M-TITLE', 'M-TITLE', 'E-TITLE', 'O']\n",
      "工 : TITLE\n",
      "学 : EDU\n",
      "高 : TITLE\n",
      "北 : ORG\n",
      "客 : TITLE\n",
      ">>>>>>>>>>\n",
      "1985年8月—1993年在国家物资局、物资部、国内贸易部金属材料流通司从事国家统配钢材中特种钢材品种的全国调拔分配工作，先后任科员、副主任科员、主任科员。\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'E-ORG', 'O', 'B-TITLE', 'M-TITLE', 'O', 'O', 'O', 'M-ORG', 'M-ORG', 'M-TITLE', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'E-ORG', 'O', 'O', 'O', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'O', 'O', 'O', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M-TITLE', 'E-TITLE', 'O', 'B-TITLE', 'M-TITLE', 'M-TITLE', 'M-TITLE', 'E-TITLE', 'O', 'B-TITLE', 'M-TITLE', 'M-TITLE', 'E-TITLE', 'O']\n",
      "国 : ORG\n",
      "物 : TITLE\n",
      "副 : TITLE\n",
      "主 : TITLE\n",
      ">>>>>>>>>>\n",
      "1993年5月—1999年5月受国内贸易部委派到国内贸易部、冶金部、天津市政府共同领导组建的北洋(天津)钢材批发交易市场任理事长助理、副总裁。\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M-ORG', 'M-ORG', 'M-TITLE', 'M-TITLE', 'M-ORG', 'O', 'O', 'O', 'M-ORG', 'M-ORG', 'M-TITLE', 'O', 'O', 'B-ORG', 'M-ORG', 'O', 'O', 'B-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'E-ORG', 'O', 'O', 'M-ORG', 'M-TITLE', 'M-ORG', 'O', 'O', 'O', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'O', 'O', 'O', 'M-ORG', 'M-ORG', 'M-ORG', 'O', 'O', 'B-TITLE', 'M-TITLE', 'M-TITLE', 'M-TITLE', 'E-TITLE', 'O', 'B-TITLE', 'M-TITLE', 'E-TITLE', 'O']\n",
      "冶 : ORG\n",
      "天 : ORG\n",
      "理 : TITLE\n",
      "副 : TITLE\n",
      ">>>>>>>>>>\n",
      "1999年5月—2010年4月任天津一德投资集团有限公司董事。\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'M-ORG', 'E-ORG', 'B-TITLE', 'E-TITLE', 'O']\n",
      "天 : ORG\n",
      "董 : TITLE\n",
      ">>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "text = pred_tag_lists[0]\r\n",
    "print(\"\".join(text))\r\n",
    "xx = pred_tag_lists[0]\r\n",
    "print(xx)\r\n",
    "\r\n",
    "for text, tag in zip(test_word_lists[:5], pred_tag_lists[:5]):\r\n",
    "    text = \"\".join(text)\r\n",
    "    print(text)\r\n",
    "    print(tag)\r\n",
    "    entities = SeqEntityScore(tag2id).get_entities_bio(tag)\r\n",
    "    for entity in entities:\r\n",
    "        entity_type, start, end = entity\r\n",
    "        print(f\"{text[start:end+1]} : {entity_type}\")\r\n",
    "    print(\">\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. 利用新数据重新训练和测试HMM、ME和CRF模型\n",
    "请在下面的Cell中：利用给定的ner_clue_data目录下的训练数据文件train.txt分别训练HMM、ME和CRF模型,并使用dev.txt文件里的数据来测试三个模型。输出每个模型在dev.txt数据上的测试结果（每个模型对应一个结果文件），在Notebook中输出每个模型对应的每种实体以及总体的Precision、Recall和F1值。（6分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在这里实现利用新数据重新训练和测试HMM、ME和CRF模型\n",
    "clue_train_word_lists, clue_train_tag_lists, clue_word2id, clue_tag2id = data_build_gluener(file_name=\"train.txt\", make_vocab=True)\n",
    "clue_dev_word_lists, clue_dev_tag_lists = data_build_gluener(file_name=\"dev.txt\", make_vocab=False)\n",
    "\n",
    "clue_test_word_lists, clue_test_tag_lists = data_build_gluener(file_name=\"test.txt\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-company': 0,\n",
       " 'I-company': 1,\n",
       " 'O': 2,\n",
       " 'B-name': 3,\n",
       " 'I-name': 4,\n",
       " 'B-game': 5,\n",
       " 'I-game': 6,\n",
       " 'B-organization': 7,\n",
       " 'I-organization': 8,\n",
       " 'B-movie': 9,\n",
       " 'I-movie': 10,\n",
       " 'B-position': 11,\n",
       " 'I-position': 12,\n",
       " 'B-address': 13,\n",
       " 'I-address': 14,\n",
       " 'B-government': 15,\n",
       " 'I-government': 16,\n",
       " 'B-scene': 17,\n",
       " 'I-scene': 18,\n",
       " 'B-book': 19,\n",
       " 'I-book': 20,\n",
       " 'S-company': 21,\n",
       " 'S-address': 22,\n",
       " 'S-name': 23,\n",
       " 'S-position': 24}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clue_tag2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "# N: 状态数，这里对应存在的标注的种类 \r\n",
    "# M: 观测数，这里对应有多少不同的字\r\n",
    "tag2id = clue_tag2id\r\n",
    "word2id = clue_word2id\r\n",
    "\r\n",
    "N, M = len(tag2id), len(word2id)\r\n",
    "# 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率\r\n",
    "A = np.zeros(shape=(N, N), dtype=float)\r\n",
    "# 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率\r\n",
    "B = np.zeros(shape=(N, M), dtype=float)\r\n",
    "# 初始状态概率  Pi[i]表示初始时刻为状态i的概率\r\n",
    "Pi = np.zeros(shape=N, dtype=float)\r\n",
    "\r\n",
    "\"\"\" 构建转移概率矩阵 \"\"\"\r\n",
    "for tag_list in clue_train_tag_lists:\r\n",
    "    seq_len = len(tag_list)\r\n",
    "    for i in range(seq_len - 1):\r\n",
    "        current_tagid = tag2id[tag_list[i]]\r\n",
    "        next_tagid = tag2id[tag_list[i+1]]\r\n",
    "        A[current_tagid][next_tagid] += 1\r\n",
    "A[A == 0.] = 1e-10  # 平滑处理\r\n",
    "A = A / np.sum(a=A, axis=1, keepdims=True)\r\n",
    "\r\n",
    "\"\"\" 构建观测概率矩阵 \"\"\"\r\n",
    "for tag_list, word_list in zip(clue_train_tag_lists, clue_train_word_lists):\r\n",
    "    assert len(tag_list) == len(word_list)\r\n",
    "    for tag, word in zip(tag_list, word_list):\r\n",
    "        tag_id = tag2id[tag]\r\n",
    "        word_id = word2id[word]\r\n",
    "        B[tag_id][word_id] += 1\r\n",
    "B[B == 0.] = 1e-10  # 平滑处理\r\n",
    "B = B / np.sum(a=B, axis=1, keepdims=True)\r\n",
    "\r\n",
    "\"\"\" 构建初始状态概率 \"\"\"\r\n",
    "for tag_list in clue_train_tag_lists:\r\n",
    "    init_tagid = tag2id[tag_list[0]]\r\n",
    "    Pi[init_tagid] += 1\r\n",
    "Pi[Pi == 0.] = 1e-10  # 平滑处理\r\n",
    "Pi = Pi / np.sum(a=Pi)\r\n",
    "\r\n",
    "\r\n",
    "\"\"\" 利用HMM识别ner_char_data目录下test.txt中的数据\"\"\"\r\n",
    "pred_tag_lists = []\r\n",
    "for word_list in clue_dev_word_lists:\r\n",
    "    pred_tag_list = viterbi(word_list, word2id, tag2id)\r\n",
    "    pred_tag_lists.append(pred_tag_list)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clue_id2tag = {(y,x)  for x,y in tag2id.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "           name     0.5456    0.5656    0.5554       465\n",
      "        address     0.3531    0.3190    0.3352       373\n",
      "   organization     0.4601    0.4714    0.4657       367\n",
      "           game     0.5460    0.6644    0.5994       295\n",
      "          scene     0.4010    0.3780    0.3892       209\n",
      "           book     0.3361    0.2597    0.2930       154\n",
      "        company     0.4592    0.4471    0.4531       378\n",
      "       position     0.5666    0.6189    0.5916       433\n",
      "     government     0.3656    0.4737    0.4127       247\n",
      "          movie     0.4518    0.4967    0.4732       151\n",
      "      avg/total     0.4689    0.4880    0.4782      3072\n",
      "                 precision    recall  f1-score   support\n",
      "         B-name     0.6826    0.7075    0.6948       465\n",
      "         I-name     0.5434    0.7111    0.6160      1021\n",
      "              O     0.9438    0.9175    0.9305     36747\n",
      "      B-address     0.5104    0.4611    0.4845       373\n",
      "      I-address     0.5780    0.6080    0.5926      1329\n",
      " B-organization     0.6011    0.6158    0.6083       367\n",
      " I-organization     0.5522    0.5253    0.5384      1087\n",
      "         B-game     0.6518    0.7932    0.7156       295\n",
      "         I-game     0.6769    0.7768    0.7234      1362\n",
      "        B-scene     0.4772    0.4498    0.4631       209\n",
      "        I-scene     0.6103    0.5249    0.5644       722\n",
      "         B-book     0.6134    0.4740    0.5348       154\n",
      "         I-book     0.6184    0.4436    0.5166       877\n",
      "      B-company     0.6359    0.6190    0.6273       378\n",
      "      I-company     0.5842    0.6517    0.6161      1315\n",
      "     B-position     0.6004    0.6559    0.6269       433\n",
      "     I-position     0.6185    0.6797    0.6476       768\n",
      "   B-government     0.4938    0.6397    0.5573       247\n",
      "   I-government     0.5676    0.7743    0.6550      1068\n",
      "        B-movie     0.5723    0.6291    0.5994       151\n",
      "        I-movie     0.6342    0.7018    0.6663       892\n",
      "      avg/total     0.8505    0.8432    0.8455     50260\n",
      "\n",
      "Confusion Matrix:\n",
      "         B-name  I-name       O B-address I-address B-organization I-organization  B-game  I-game B-scene I-scene  B-book  I-book B-company I-company B-position I-position B-government I-government B-movie I-movie \n",
      " B-name     329      19      70       3       3      17       1       2       1       1       1       0       1      10       1       0       3       0       0       1       2 \n",
      " I-name       5     726     137       2      20       0      40       0      21       0       4       0       2       1      51       0       0       0       0       1      11 \n",
      "      O      64     249   33717      83     344      75     228      59     199      44     123      16     122      79     412     135     239      83     367      16      91 \n",
      "B-address      13       3      74     172      37      16       2       1       0      26       2       2       1       3       4       1       0      14       1       1       0 \n",
      "I-address       2      59     222      14     808       4      53       0       9       2      79       0       8       2      27       1       2       1      35       0       1 \n",
      "B-organization      15       6      44       5       1     226      30      10       0       3       0       0       1       9       0       1       0      15       1       0       0 \n",
      "I-organization       3      60     173       0      19       5     571       3      41       0       7       0       2       1      36      10      24       4     128       0       0 \n",
      " B-game       1       0      12       0       0       0       1     234      12       0       0      10       1       0       0       0       0       1       0      23       0 \n",
      " I-game       1       5      73       1       1       0       1       4    1058       1       1       0      53       0       0       0       0       0       3       1     159 \n",
      "B-scene       8       0      57      17       7      11       0       0       0      94       8       0       0       2       0       0       0       3       0       0       2 \n",
      "I-scene       1      36     129       4      63       3      39       0       0      17     379       0       0       1      32       2       4       0       6       0       6 \n",
      " B-book       5       0      37       2       1       1       0      14       0       0       0      73       1       3       0       0       1       0       0      16       0 \n",
      " I-book       1      22     260       6      23       1       5       1      61       3       5       7     389       1       9       1       1       1       5       4      71 \n",
      "B-company      25       3      69      15       1       6       0       6       0       3       0       2       0     234       7       0       0       4       0       2       1 \n",
      "I-company       1     130     182       2      33       0      13       0      39       0       7       0       3      12     857       4       4       2      17       0       9 \n",
      "B-position       1       3      87       2       1       2       4       2       3       0       0       2       0       1       2     284      25       8       6       0       0 \n",
      "I-position       0       5     136       1       5       1      11       0       7       0       0       0       8       0       3      26     522       2      41       0       0 \n",
      "B-government       0       0      42       7       0       6       2       3       1       1       0       0       0       6       0       1       0     158      20       0       0 \n",
      "I-government       2       2      86       0      26       0      28       1      13       1       3       0       0       3      26       7      19      24     827       0       0 \n",
      "B-movie       2       0      19       1       0       1       0      16       2       0       0       7       0       0       0       0       0       0       0      95       8 \n",
      "I-movie       3       8      99       0       5       1       5       3      96       1       2       0      37       0       0       0       0       0       0       6     626 \n"
     ]
    }
   ],
   "source": [
    "# 实体级别\r\n",
    "print_score(clue_id2tag,clue_dev_tag_lists, pred_tag_lists)\r\n",
    "# 字符级别\r\n",
    "metrics = Metrics(clue_dev_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "metrics.report_scores()\r\n",
    "metrics.report_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练CRF模型\r\n",
    "crf_model = CRFModel()\r\n",
    "crf_model.train(clue_train_word_lists, clue_train_tag_lists)\r\n",
    "\r\n",
    "pred_tag_lists = crf_model.test(clue_dev_word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "           name     0.7778    0.6925    0.7327       465\n",
      "        address     0.6021    0.4665    0.5257       373\n",
      "   organization     0.7934    0.7221    0.7561       367\n",
      "           game     0.7845    0.7898    0.7872       295\n",
      "          scene     0.7211    0.5072    0.5955       209\n",
      "           book     0.7742    0.6234    0.6906       154\n",
      "        company     0.7822    0.7222    0.7510       378\n",
      "       position     0.8226    0.7067    0.7602       433\n",
      "     government     0.7924    0.7571    0.7743       247\n",
      "          movie     0.7279    0.7086    0.7181       151\n",
      "      avg/total     0.7638    0.6735    0.7158      3072\n",
      "                 precision    recall  f1-score   support\n",
      "         B-name     0.8382    0.7462    0.7895       465\n",
      "         I-name     0.7781    0.7522    0.7649      1021\n",
      "              O     0.9461    0.9760    0.9608     36747\n",
      "      B-address     0.7232    0.5603    0.6314       373\n",
      "      I-address     0.7514    0.7050    0.7275      1329\n",
      " B-organization     0.8293    0.7548    0.7903       367\n",
      " I-organization     0.7785    0.6403    0.7027      1087\n",
      "         B-game     0.8350    0.8407    0.8378       295\n",
      "         I-game     0.8382    0.8253    0.8317      1362\n",
      "        B-scene     0.7483    0.5263    0.6180       209\n",
      "        I-scene     0.7903    0.5845    0.6720       722\n",
      "         B-book     0.7823    0.6299    0.6978       154\n",
      "         I-book     0.7698    0.6442    0.7014       877\n",
      "      B-company     0.8539    0.7884    0.8198       378\n",
      "      I-company     0.7823    0.7817    0.7820      1315\n",
      "     B-position     0.8280    0.7113    0.7652       433\n",
      "     I-position     0.8481    0.6979    0.7657       768\n",
      "   B-government     0.8517    0.8138    0.8323       247\n",
      "   I-government     0.8184    0.8230    0.8207      1068\n",
      "        B-movie     0.7619    0.7417    0.7517       151\n",
      "        I-movie     0.7869    0.7825    0.7847       892\n",
      "      avg/total     0.9062    0.9098    0.9069     50260\n",
      "\n",
      "Confusion Matrix:\n",
      "         B-name  I-name       O B-address I-address B-organization I-organization  B-game  I-game B-scene I-scene  B-book  I-book B-company I-company B-position I-position B-government I-government B-movie I-movie \n",
      " B-name     347      10      90       4       1       4       1       1       0       0       0       1       1       4       0       0       0       0       1       0       0 \n",
      " I-name       3     768     194       0       9       0      12       0       6       0       0       0       5       0      22       0       0       0       2       0       0 \n",
      "      O      25      89   35865      39     162      30      76       8      37      13      37       9      52      19     102      34      52      13      73       3       9 \n",
      "B-address       5       1      96     209      25      10       0       0       0      18       2       0       0       3       3       0       0       1       0       0       0 \n",
      "I-address       1      10     248       5     937       0      36       0       0       0      53       1      10       3      23       0       0       0       2       0       0 \n",
      "B-organization       9       0      47       5       0     277       4       2       0       1       0       0       0       9       1       2       0       9       1       0       0 \n",
      "I-organization       0      21     193       0      16       2     696       2      12       0       4       0       0       0      58       0      12       0      71       0       0 \n",
      " B-game       1       0      26       0       0       0       0     248       5       0       0       7       0       0       0       0       0       0       0       8       0 \n",
      " I-game       1      11      97       0       0       0       0       6    1124       0       0       0      48       0       0       0       0       0       0       2      73 \n",
      "B-scene       4       0      58      17       6       1       0       0       0     110       9       0       0       2       0       0       0       0       0       1       1 \n",
      "I-scene       0      25     148       1      70       0       1       0       0       3     422       0       0       1      37       0       0       0       0       0      14 \n",
      " B-book       4       0      22       0       0       1       0      11       0       0       0      97       1       3       0       0       0       0       0      15       0 \n",
      " I-book       1      12     145       0       0       1       5       0      55       0       0       2     565       0       8       0       0       0       0       0      83 \n",
      "B-company       9       0      47       6       0       1       0       1       0       2       0       1       1     298       7       3       0       1       0       1       0 \n",
      "I-company       0      34     179       1      14       0      14       0       9       0       7       0       4       2    1028       1      12       0       3       0       7 \n",
      "B-position       1       1      86       0       0       2       1       1       0       0       0       0       2       2       1     308      17       7       4       0       0 \n",
      "I-position       1       3     143       0       0       0      16       0       3       0       0       0       2       0       7      22     536       0      35       0       0 \n",
      "B-government       0       0      30       1       1       5       0       0       0       0       0       0       0       3       1       2       0     201       3       0       0 \n",
      "I-government       0       0     130       0       4       0      32       0       0       0       0       0       0       0      16       0       3       4     879       0       0 \n",
      "B-movie       1       0      13       1       0       0       0      15       1       0       0       6       0       0       0       0       0       0       0     112       2 \n",
      "I-movie       1       2      50       0       2       0       0       2      89       0       0       0      43       0       0       0       0       0       0       5     698 \n"
     ]
    }
   ],
   "source": [
    "# 实体级别\r\n",
    "print_score(clue_id2tag,clue_dev_tag_lists, pred_tag_lists)\r\n",
    "# 字符级别\r\n",
    "metrics = Metrics(clue_dev_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "metrics.report_scores()\r\n",
    "metrics.report_confusion_matrix()\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "最大熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done train_tag_lists_flatten\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "           name     0.5785    0.4753    0.5218       465\n",
      "        address     0.2986    0.2306    0.2602       373\n",
      "   organization     0.6300    0.5613    0.5937       367\n",
      "           game     0.6339    0.6339    0.6339       295\n",
      "          scene     0.3608    0.1675    0.2288       209\n",
      "           book     0.3279    0.2597    0.2899       154\n",
      "        company     0.4841    0.4021    0.4393       378\n",
      "       position     0.7957    0.6028    0.6859       433\n",
      "     government     0.5130    0.3198    0.3940       247\n",
      "          movie     0.3276    0.2517    0.2846       151\n",
      "      avg/total     0.5386    0.4248    0.4750      3072\n",
      "                 precision    recall  f1-score   support\n",
      "         B-name     0.8010    0.6581    0.7226       465\n",
      "         I-name     0.6560    0.6650    0.6605      1021\n",
      "              O     0.9169    0.9719    0.9436     36747\n",
      "      B-address     0.5938    0.4584    0.5174       373\n",
      "      I-address     0.6236    0.5260    0.5706      1329\n",
      " B-organization     0.7706    0.6866    0.7262       367\n",
      " I-organization     0.7440    0.5722    0.6469      1087\n",
      "         B-game     0.8000    0.8000    0.8000       295\n",
      "         I-game     0.7727    0.7562    0.7644      1362\n",
      "        B-scene     0.6598    0.3062    0.4183       209\n",
      "        I-scene     0.6404    0.4169    0.5050       722\n",
      "         B-book     0.7049    0.5584    0.6232       154\n",
      "         I-book     0.7435    0.4230    0.5392       877\n",
      "      B-company     0.7389    0.6138    0.6705       378\n",
      "      I-company     0.7192    0.6487    0.6821      1315\n",
      "     B-position     0.8384    0.6351    0.7227       433\n",
      "     I-position     0.8587    0.7044    0.7740       768\n",
      "   B-government     0.6818    0.4251    0.5237       247\n",
      "   I-government     0.7272    0.7088    0.7179      1068\n",
      "        B-movie     0.7069    0.5430    0.6142       151\n",
      "        I-movie     0.6748    0.5561    0.6097       892\n",
      "      avg/total     0.8641    0.8729    0.8654     50260\n"
     ]
    }
   ],
   "source": [
    "clue_train_tag_lists_flatten = [x for train_tag_list in clue_train_tag_lists for x in train_tag_list]\r\n",
    "print(\"done train_tag_lists_flatten\")\r\n",
    "# train\r\n",
    "logistic_model = MaxEntropyModel()\r\n",
    "logistic_model.train(clue_train_word_lists, clue_train_tag_lists_flatten)\r\n",
    "\r\n",
    "# test\r\n",
    "pred_tag_lists = logistic_model.test(clue_dev_word_lists)\r\n",
    "\r\n",
    "print_score(clue_id2tag, clue_dev_tag_lists, pred_tag_lists)\r\n",
    "\r\n",
    "metrics = Metrics(clue_dev_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "metrics.report_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 7. 训练和测试BiLSTM+CRF模型\n",
    "请在下面的Cell中：基于百度Paddle框架，利用ner_clue_data目录下的训练数据文件train.txt训练一个BiLSTM+CRF模型,并使用dev.txt文件里的数据来测试模型。输出模型在dev.txt数据上的测试结果，在Notebook中输出模型对应的每种实体以及总体的Precision、Recall和F1值。（2分）\n",
    "\n",
    "参考项目：https://aistudio.baidu.com/aistudio/projectdetail/4877149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在这里基于百度Paddle框架，实现一个实体识别的BiLSTM+CRF模型\n",
    "\n",
    "from score import SeqEntityScore\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from score import SeqEntityScore\n",
    "import paddle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.1 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_word_lists, train_tag_lists, word2id, tag2id \r\n",
    "\"\"\" 读取并处理 ner_char_data 目录下的数据文件 \"\"\"\r\n",
    "import json\r\n",
    "    \r\n",
    "def data_build_gluener(file_name:str, make_vocab=True):\r\n",
    "    word_lists = []\r\n",
    "    tag_lists = []\r\n",
    "    with open('./ner_clue_data/' + file_name, 'r', encoding='utf-8') as f:\r\n",
    "\r\n",
    "        for line in f:\r\n",
    "            json_data = {}\r\n",
    "            line = json.loads(line.strip())\r\n",
    "            text = line['text']\r\n",
    "            label_items = line['label']\r\n",
    "            \r\n",
    "            # 标注\r\n",
    "            labels = ['O']*len(text)\r\n",
    "            if(label_items != None):\r\n",
    "                for key,value in label_items.items():\r\n",
    "                    for name, index in value.items():\r\n",
    "                        for start_idx, end_idx in index:\r\n",
    "                            assert text[start_idx:end_idx + 1] == name\r\n",
    "                            if(start_idx == end_idx):\r\n",
    "                                labels[start_idx] = 'S-' + key\r\n",
    "                            else:\r\n",
    "                                labels[start_idx] = 'B-'+ key\r\n",
    "                                labels[start_idx+1: end_idx+1] = ['I-'+key]*(end_idx - start_idx)\r\n",
    "            word_lists.append(list(text))\r\n",
    "            tag_lists.append(labels)\r\n",
    "\r\n",
    "    if make_vocab == True:\r\n",
    "        word2id = {}\r\n",
    "        for word_list in word_lists:\r\n",
    "            for word in word_list:\r\n",
    "                if word not in word2id:\r\n",
    "                    word2id[word] = len(word2id)\r\n",
    "        tag2id = {}\r\n",
    "        for tag_list in tag_lists:\r\n",
    "            for tag in tag_list:\r\n",
    "                if tag not in tag2id:\r\n",
    "                    tag2id[tag] = len(tag2id)\r\n",
    "        return word_lists, tag_lists, word2id, tag2id\r\n",
    "\r\n",
    "    return word_lists, tag_lists\r\n",
    "\r\n",
    "# 加载词典\r\n",
    "import os\r\n",
    "def load_dict(dict_name):\r\n",
    "    assert dict_name in [\"tag\", \"vocab\"]\r\n",
    "    path = './ner_clue_data/'\r\n",
    "    data_path = os.path.join(path, dict_name+\".dict\")\r\n",
    "    data_dict = {}\r\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\r\n",
    "        lines = [item.strip().split(\"\\t\") for item in f.readlines()]\r\n",
    "        data_dict = dict([(item[1], int(item[0])) for item in lines])\r\n",
    "\r\n",
    "    return data_dict              \r\n",
    "\r\n",
    "\r\n",
    "# clue_train_word_lists, clue_train_tag_lists, clue_word2id, clue_tag2id = data_build_gluener(file_name=\"train.txt\", make_vocab=True)\r\n",
    "# clue_dev_word_lists, clue_dev_tag_lists = data_build_gluener(file_name=\"dev.txt\", make_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clue_train_word_lists, clue_train_tag_lists = data_build_gluener(file_name=\"train.txt\", make_vocab=False)\r\n",
    "clue_dev_word_lists, clue_dev_tag_lists = data_build_gluener(file_name=\"dev.txt\", make_vocab=False)\r\n",
    "\r\n",
    "clue_word2id = load_dict(dict_name=\"vocab\")\r\n",
    "clue_tag2id = load_dict(dict_name=\"tag\")\r\n",
    "clue_id2tag = dict([items[1], items[0]] for items in clue_tag2id.items())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-address',\n",
       " 2: 'B-book',\n",
       " 3: 'B-company',\n",
       " 4: 'B-game',\n",
       " 5: 'B-government',\n",
       " 6: 'B-movie',\n",
       " 7: 'B-name',\n",
       " 8: 'B-organization',\n",
       " 9: 'B-position',\n",
       " 10: 'B-scene',\n",
       " 11: 'I-address',\n",
       " 12: 'I-book',\n",
       " 13: 'I-company',\n",
       " 14: 'I-game',\n",
       " 15: 'I-government',\n",
       " 16: 'I-movie',\n",
       " 17: 'I-name',\n",
       " 18: 'I-organization',\n",
       " 19: 'I-position',\n",
       " 20: 'I-scene',\n",
       " 21: 'S-address',\n",
       " 22: 'S-book',\n",
       " 23: 'S-company',\n",
       " 24: 'S-game',\n",
       " 25: 'S-government',\n",
       " 26: 'S-movie',\n",
       " 27: 'S-name',\n",
       " 28: 'S-organization',\n",
       " 29: 'S-position',\n",
       " 30: 'S-scene'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clue_id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utils\r\n",
    "import paddle\r\n",
    "\r\n",
    "def conver_word_to_id(word_list, word2id):\r\n",
    "    return [word2id.get(word) for word in word_list]\r\n",
    "\r\n",
    "def conver_tag_to_id(tag_lists, tag2id):\r\n",
    "    return conver_word_to_id(tag_lists, tag2id)\r\n",
    "\r\n",
    "def zip_data (word_lists, tag_lists, word2id, tag2id):\r\n",
    "    \"\"\"\r\n",
    "    [(words, tags, len),(words, tags, len)...]\r\n",
    "    \"\"\"\r\n",
    "    return [(conver_word_to_id(x,word2id), conver_tag_to_id(y, tag2id),len(x)) for x,y in zip (word_lists,tag_lists)]\r\n",
    "\r\n",
    "train_set = zip_data(clue_train_word_lists, clue_train_tag_lists, clue_word2id, clue_tag2id)\r\n",
    "dev_set = zip_data(clue_dev_word_lists, clue_dev_tag_lists, clue_word2id, clue_tag2id)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DatasetLoader(object):\r\n",
    "    def __init__(self, data, batch_size, shuffle=False, sort=True, drop_last=False):\r\n",
    "        self.examples = data\r\n",
    "        self.shuffle = shuffle\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.sort = sort\r\n",
    "        self.drop_last = drop_last\r\n",
    "        self.split_data()\r\n",
    "    \r\n",
    "    # 将数据分成mini-batch\r\n",
    "    def split_data(self):\r\n",
    "        # 根据样本的长度对数据进行排序\r\n",
    "        if self.sort:\r\n",
    "            self.examples = sorted(self.examples, key=lambda x: x[2], reverse=True)\r\n",
    "        if self.shuffle:\r\n",
    "            indices = list(range(len(self.examples)))\r\n",
    "            random.shuffle(indices)\r\n",
    "            self.examples = [self.examples[i] for i in indices]\r\n",
    "        self.batch_data = [self.examples[i:i + self.batch_size] for i in range(0, len(self.examples), self.batch_size)]\r\n",
    "        if self.drop_last and len(self.batch_data[-1]) < batch_size:\r\n",
    "            self.batch_data = self.batch_data[:-1]\r\n",
    "\r\n",
    "    def padding_sequence(self, batch, batch_size, mask=None):\r\n",
    "        # 固定batch中样本的长度，将短的样本padding到最长样本的长度\r\n",
    "        tokens_list, tags_list, lens = batch\r\n",
    "        max_len = max(lens)\r\n",
    "        batch_size = len(lens)\r\n",
    "        batch_token = paddle.full(shape=[batch_size, max_len], fill_value=0, dtype=\"int64\")\r\n",
    "        batch_tag = paddle.full(shape=[batch_size, max_len], fill_value=0, dtype=\"int64\")\r\n",
    "        batch_mask = paddle.full(shape=[batch_size, max_len], fill_value=0, dtype=\"int64\")\r\n",
    "        for i in range(batch_size):\r\n",
    "\r\n",
    "            batch_token[i, :lens[i]] = paddle.to_tensor(tokens_list[i], dtype=\"int64\")\r\n",
    "            batch_tag[i, :lens[i]] = paddle.to_tensor(tags_list[i], dtype=\"int64\")\r\n",
    "\r\n",
    "            if mask:\r\n",
    "                batch_mask[i, :lens[i]] = paddle.to_tensor([1] * lens[i], dtype=\"int64\") \r\n",
    "        if mask:\r\n",
    "            return batch_token, batch_tag, batch_mask\r\n",
    "\r\n",
    "        return batch_token, batch_tag\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.batch_data)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        # 每次调用该函数，则返回一个batch\r\n",
    "        # batch中每条样本包含三部分数据：text，tag, len\r\n",
    "        batch = self.batch_data[index]\r\n",
    "        batch_size = len(batch)\r\n",
    "        # batch变成三个list，依次是text，tag, len\r\n",
    "        batch = list(zip(*batch))\r\n",
    "        batch_ids, batch_tags, batch_mask = self.padding_sequence(batch, batch_size, mask=True)\r\n",
    "        batch_lens = paddle.to_tensor(batch[2])\r\n",
    "\r\n",
    "        return (batch_ids, batch_tags, batch_mask, batch_lens)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.2 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp.layers.crf  as crf\r\n",
    "\r\n",
    "class NERModel(paddle.nn.Layer):\r\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, label2id, n_layers=2, drop_p = 0.1 ):\r\n",
    "        super(NERModel, self).__init__()\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.label2id = label2id\r\n",
    "\r\n",
    "        self.embedding = paddle.nn.Embedding(vocab_size, embedding_size)\r\n",
    "        self.bilstm = paddle.nn.LSTM(input_size = embedding_size, hidden_size = hidden_size,direction=\"bidirectional\", num_layers=n_layers, dropout=drop_p )\r\n",
    "        self.layer_norm = paddle.nn.LayerNorm(hidden_size * 2)\r\n",
    "        self.dropout_emb = paddle.nn.Dropout(drop_p)\r\n",
    "        self.classifier = paddle.nn.Linear(hidden_size*2 , len(label2id)+2) # add START and STOP tag\r\n",
    "\r\n",
    "        self.crf = crf.LinearChainCrf(len(label2id), crf_lr = 0.001, with_start_stop_tag = True)\r\n",
    "        self.crf_loss = crf.LinearChainCrfLoss(self.crf)\r\n",
    "        self.viterbi_decoder = crf.ViterbiDecoder(self.crf.transitions)\r\n",
    "    \r\n",
    "    def forward(self, input_id, input_mask):\r\n",
    "        # input_id: [batch_size, seq_len]\r\n",
    "        embs = self.embedding(input_id) #[batch_size,seq_len, embedding_sie]\r\n",
    "        embs = self.dropout_emb(embs)\r\n",
    "        embs = embs*paddle.to_tensor(input_mask, dtype = 'float32').unsqueeze(2)\r\n",
    "\r\n",
    "        last_layer_hiddens, _ =  self.bilstm(embs)\r\n",
    "        last_layer_hiddens = self.layer_norm(last_layer_hiddens)\r\n",
    "        features = self.classifier(last_layer_hiddens)\r\n",
    "        \r\n",
    "        return features\r\n",
    "    \r\n",
    "    def forward_loss(self, input_ids, input_mask, input_lens, input_tags=None):\r\n",
    "        features = self.forward(input_ids, input_mask)\r\n",
    "        if input_tags is not None:\r\n",
    "            return features, self.crf_loss(features, input_lens,\"\",input_tags)\r\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.3 训练与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu:0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 参数\r\n",
    "n_epochs = 100\r\n",
    "batch_size = 32\r\n",
    "vocab_size = len(clue_word2id.keys())\r\n",
    "embedding_size = 128\r\n",
    "hidden_size = 384\r\n",
    "n_layers = 2\r\n",
    "dropout_rate = 0.1\r\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DatasetLoader(train_set, batch_size, False, True)\r\n",
    "test_loader = DatasetLoader(dev_set, batch_size,  False ,True)\r\n",
    "\r\n",
    "\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "ner_model = NERModel(vocab_size=vocab_size, embedding_size=embedding_size,\r\n",
    "                     hidden_size=hidden_size,label2id=clue_tag2id, n_layers=n_layers, drop_p=dropout_rate)\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.99,\r\n",
    "                                  parameters=ner_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-address',\n",
       " 2: 'B-book',\n",
       " 3: 'B-company',\n",
       " 4: 'B-game',\n",
       " 5: 'B-government',\n",
       " 6: 'B-movie',\n",
       " 7: 'B-name',\n",
       " 8: 'B-organization',\n",
       " 9: 'B-position',\n",
       " 10: 'B-scene',\n",
       " 11: 'I-address',\n",
       " 12: 'I-book',\n",
       " 13: 'I-company',\n",
       " 14: 'I-game',\n",
       " 15: 'I-government',\n",
       " 16: 'I-movie',\n",
       " 17: 'I-name',\n",
       " 18: 'I-organization',\n",
       " 19: 'I-position',\n",
       " 20: 'I-scene',\n",
       " 21: 'S-address',\n",
       " 22: 'S-book',\n",
       " 23: 'S-company',\n",
       " 24: 'S-game',\n",
       " 25: 'S-government',\n",
       " 26: 'S-movie',\n",
       " 27: 'S-name',\n",
       " 28: 'S-organization',\n",
       " 29: 'S-position',\n",
       " 30: 'S-scene'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clue_id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "# 模型评估\r\n",
    "def evaluate(model,test_loader):\r\n",
    "    # 定义统计评估指标的类\r\n",
    "    metric = SeqEntityScore(clue_id2tag)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    with paddle.no_grad():\r\n",
    "        for step, batch in enumerate(test_loader):\r\n",
    "            # 获取数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            \r\n",
    "            # 前向计算，得出发射分数\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "\r\n",
    "            # 根据发射分数，利用CRF进行解码\r\n",
    "            scores, pred_paths = model.viterbi_decoder(features, batch_lens)\r\n",
    "\r\n",
    "            # 将这些预测的标签序列进行id2tag，即转换为相应的标签\r\n",
    "            pred_paths = [[clue_id2tag[int(tag_id)] for tag_id in tag_seq] for tag_seq in pred_paths]\r\n",
    "            \r\n",
    "            # 根据文本序列的真实长度，对真实标签序列进行截断\r\n",
    "            batch_tags = batch_tags.numpy().tolist()\r\n",
    "            real_paths = [tag_seq[:tag_len] for tag_seq, tag_len in zip(batch_tags, batch_lens)]\r\n",
    "\r\n",
    "            # 更新统计指标相关数据\r\n",
    "            metric.update(pred_paths=pred_paths, real_paths=real_paths)\r\n",
    "\r\n",
    "    # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metric.get_result()\r\n",
    "    #format_print(result)\r\n",
    "    metric.format_print(result)\r\n",
    "\r\n",
    "    return result \r\n",
    "\r\n",
    "from evaluating import Metrics\r\n",
    "from score import SeqEntityScore\r\n",
    "def evaluate_detail(model,test_loader):\r\n",
    "    # 定义统计评估指标的类\r\n",
    "    metric_ent = SeqEntityScore(clue_id2tag)\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    test_tag_lists = []\r\n",
    "    pred_tag_lists = []\r\n",
    "    with paddle.no_grad():\r\n",
    "        for step, batch in enumerate(test_loader):\r\n",
    "            # 获取数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            \r\n",
    "            # 前向计算，得出发射分数\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "\r\n",
    "            # 根据发射分数，利用CRF进行解码\r\n",
    "            scores, pred_paths = model.viterbi_decoder(features, batch_lens)\r\n",
    "\r\n",
    "            # 将这些预测的标签序列进行id2tag，即转换为相应的标签\r\n",
    "            pred_paths = [[clue_id2tag[int(tag_id)] for tag_id in tag_seq] for tag_seq in pred_paths]\r\n",
    "            \r\n",
    "            # 根据文本序列的真实长度，对真实标签序列进行截断\r\n",
    "            batch_tags = batch_tags.numpy().tolist()\r\n",
    "            real_paths = [tag_seq[:tag_len] for tag_seq, tag_len in zip(batch_tags, batch_lens)]\r\n",
    "\r\n",
    "            # 更新统计指标相关数据\r\n",
    "            metric_ent.update(pred_paths=pred_paths, real_paths=real_paths)\r\n",
    "\r\n",
    "            test_tag_lists.extend([clue_id2tag[i] for real_path in real_paths for i in real_path  ] )\r\n",
    "            pred_tag_lists.extend([pred_path for pred_path in pred_paths])\r\n",
    "    \r\n",
    "    # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metric_ent.get_result()\r\n",
    "    metric_ent.report_scores(result)\r\n",
    "    \r\n",
    "    metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "    metrics.report_scores()\r\n",
    "    metrics.report_confusion_matrix()\r\n",
    "\r\n",
    "    return result \r\n",
    "    \r\n",
    "\r\n",
    "# 模型训练\r\n",
    "def train(model, train_loader, test_loader):\r\n",
    "\r\n",
    "    for epoch in range(1, 1 + n_epochs):\r\n",
    "       \r\n",
    "        model.train()\r\n",
    "        print(f\"Epoch {epoch}/{n_epochs}\")\r\n",
    "        for step, batch in enumerate(train_loader):\r\n",
    "            # 获取batch中的数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            # 执行模型的前向计算，并计算出损失\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "            loss = paddle.mean(loss)\r\n",
    "            \r\n",
    "            # 梯度计算和反向参数更新\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_gradients()\r\n",
    "\r\n",
    "            # 训练过程中打印信息\r\n",
    "            if step % 20 ==0:\r\n",
    "                print(f\"epoch: {epoch}, step: {step}, loss: {loss.numpy()[0]}\")\r\n",
    "        \r\n",
    "        # 模型评估\r\n",
    "        evaluate(model, test_loader)\r\n",
    "    evaluate_detail(model, test_loader)\r\n",
    "    \r\n",
    "train(ner_model, train_loader, test_loader)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.4 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型保存的名称\r\n",
    "model_name = \"ner_model\"\r\n",
    "# 保存模型\r\n",
    "paddle.save(ner_model.state_dict(), \"{}.pdparams\".format(model_name))\r\n",
    "paddle.save(optimizer.state_dict(), \"{}.optparams\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load\r\n",
    "layer_state_dict = paddle.load(\"ner_model.pdparams\")\r\n",
    "opt_state_dict = paddle.load(\"ner_model.optparams\")\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "ner_model = NERModel(vocab_size=vocab_size, embedding_size=embedding_size,\r\n",
    "                     hidden_size=hidden_size,label2id=clue_tag2id, n_layers=n_layers, drop_p=dropout_rate)\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.99,\r\n",
    "                                  parameters=ner_model.parameters())\r\n",
    "                                \r\n",
    "ner_model.set_state_dict(layer_state_dict)\r\n",
    "optimizer.set_state_dict(opt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = SeqEntityScore(clue_id2tag)\r\n",
    "def infer(model, text, word2id,tag2id):\r\n",
    "    model.eval()\r\n",
    "    # 数据处理\r\n",
    "    tokens = [word2id.get(w) for w in list(text)]\r\n",
    "    tokens_len = len(tokens)\r\n",
    "\r\n",
    "    # 构造输入模型的数据\r\n",
    "    tokens = paddle.to_tensor(tokens, dtype=\"int64\").unsqueeze(0)\r\n",
    "    tokens_mask = paddle.to_tensor([1] * tokens_len, dtype=\"int64\").unsqueeze(0)\r\n",
    "    tokens_len = paddle.to_tensor(tokens_len, dtype=\"int64\")\r\n",
    "\r\n",
    "    # 计算发射分数\r\n",
    "    features = model.forward_loss(tokens, tokens_mask, tokens_len)\r\n",
    "\r\n",
    "    # 根据发射分数进行解码\r\n",
    "    _, pred_paths = model.viterbi_decoder(features, tokens_len)\r\n",
    "\r\n",
    "    print(pred_paths[0])\r\n",
    "    print([tag2id.get(int(x)) for x in pred_paths[0] ])\r\n",
    "    print(text)\r\n",
    "    # 解析路径中的实体\r\n",
    "    entities = SeqEntityScore(tag2id).get_entities_bios(pred_paths[0])\r\n",
    "    print(entities)\r\n",
    "    for entity in entities:\r\n",
    "        entity_type, start, end = entity\r\n",
    "        print(f\"{text[start:end+1]} : {entity_type}\")\r\n",
    "\r\n",
    "\r\n",
    "text=\"今年1月中国光大银行沈阳银行致电，称由于他的信用记录良好，银行可把普通信用卡免费升级为白金卡\"\r\n",
    "infer(ner_model, text, clue_word2id, clue_id2tag)\r\n",
    "    \r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['company', 4, 13]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [2, 2, 2, 2, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\n",
    "metric.get_entities_bio(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 去除CRF层重新训练\n",
    "结果反而比CRF高？\n",
    "看下一节吧，下一节直接在BiLSTM+CRF模型的基础上去除CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "\r\n",
    "class NERModel_withoutCRF(paddle.nn.Layer):\r\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, label2id, n_layers=2, drop_p = 0.1 ):\r\n",
    "        super(NERModel_withoutCRF, self).__init__()\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.label2id = label2id\r\n",
    "\r\n",
    "        self.embedding = paddle.nn.Embedding(vocab_size, embedding_size)\r\n",
    "        self.bilstm = paddle.nn.LSTM(input_size = embedding_size, hidden_size = hidden_size,direction=\"bidirectional\", num_layers=n_layers, dropout=drop_p )\r\n",
    "        self.layer_norm = paddle.nn.LayerNorm(hidden_size * 2)\r\n",
    "        self.dropout_emb = paddle.nn.Dropout(drop_p)\r\n",
    "        self.classifier = paddle.nn.Linear(hidden_size*2 , len(label2id)+2) # add START and STOP tag\r\n",
    "        \r\n",
    "    \r\n",
    "    def decoder(self, features):\r\n",
    "        \"\"\"\r\n",
    "        features [N, seqlen, tag_size]\r\n",
    "        \"\"\"\r\n",
    "        return paddle.argmax(features, axis= -1) #(N,seqlen)\r\n",
    "    \r\n",
    "    def forward(self, input_id, input_mask):\r\n",
    "        # input_id: [batch_size, seq_len]\r\n",
    "        embs = self.embedding(input_id) #[batch_size,seq_len, embedding_sie]\r\n",
    "        embs = self.dropout_emb(embs)\r\n",
    "        embs = embs*paddle.to_tensor(input_mask, dtype = 'float32').unsqueeze(2)\r\n",
    "\r\n",
    "        last_layer_hiddens, _ =  self.bilstm(embs)\r\n",
    "        last_layer_hiddens = self.layer_norm(last_layer_hiddens)\r\n",
    "        features = self.classifier(last_layer_hiddens)\r\n",
    "        \r\n",
    "        return features\r\n",
    "    \r\n",
    "    def forward_loss(self, input_ids, input_mask, input_lens, input_tags=None):\r\n",
    "        features = self.forward(input_ids, input_mask)\r\n",
    "        input_tags = paddle.unsqueeze(input_tags, axis=-1)\r\n",
    "        loss = paddle.nn.functional.softmax_with_cross_entropy(logits=features, label=input_tags)\r\n",
    "        # if input_tags is not None:\r\n",
    "        #     return features, self.crf_loss(features, input_lens,\"\",input_tags)\r\n",
    "        return features,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 参数\r\n",
    "n_epochs = 50\r\n",
    "batch_size = 32\r\n",
    "vocab_size = len(clue_word2id.keys())\r\n",
    "embedding_size = 128\r\n",
    "hidden_size = 384\r\n",
    "n_layers = 2\r\n",
    "dropout_rate = 0.1\r\n",
    "learning_rate = 0.001\r\n",
    "\r\n",
    "\r\n",
    "train_loader = DatasetLoader(train_set, batch_size, False, True)\r\n",
    "test_loader = DatasetLoader(dev_set, batch_size,  False ,True)\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model_withouCRF = NERModel_withoutCRF(vocab_size=vocab_size, embedding_size=embedding_size,\r\n",
    "                     hidden_size=hidden_size,label2id=clue_tag2id, n_layers=n_layers, drop_p=dropout_rate)\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.99,\r\n",
    "                                  parameters=model_withouCRF.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I-game     944      14      25       9      31      11      40       9      33       7      24       6      28       5      17       5      23       5      24      20      82 \r"
     ]
    }
   ],
   "source": [
    "\r\n",
    "# 模型训练\r\n",
    "def train(model, train_loader, test_loader):\r\n",
    "\r\n",
    "    for epoch in range(1, 1 + n_epochs):\r\n",
    "        model.train()\r\n",
    "        print(f\"Epoch {epoch}/{n_epochs}\")\r\n",
    "        for step, batch in enumerate(train_loader):\r\n",
    "            # 获取batch中的数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            # 执行模型的前向计算，并计算出损失\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "            loss = paddle.mean(loss)\r\n",
    "            \r\n",
    "            # 梯度计算和反向参数更新\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_gradients()\r\n",
    "\r\n",
    "            # 训练过程中打印信息\r\n",
    "            if step % 20 ==0:\r\n",
    "                print(f\"epoch: {epoch}, step: {step}, loss: {loss.numpy()[0]}\")\r\n",
    "        \r\n",
    "        # 模型评估\r\n",
    "        evaluate(model, test_loader)\r\n",
    "    \r\n",
    "    evaluate_detail(model, test_loader)\r\n",
    "\r\n",
    "# 模型评估\r\n",
    "def evaluate(model,test_loader):\r\n",
    "    # 定义统计评估指标的类\r\n",
    "    metric = SeqEntityScore(clue_id2tag)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    with paddle.no_grad():\r\n",
    "        for step, batch in enumerate(test_loader):\r\n",
    "            # 获取数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            \r\n",
    "            # 前向计算，得出发射分数\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "\r\n",
    "            # 根据发射分数，利用CRF进行解码\r\n",
    "            pred_paths = model.decoder(features) #(N,seqlen)\r\n",
    "\r\n",
    "            # 将这些预测的标签序列进行id2tag，即转换为相应的标签\r\n",
    "            pred_paths = [[clue_id2tag[int(tag_id)] for tag_id in tag_seq] for tag_seq in pred_paths]\r\n",
    "            \r\n",
    "            # 根据文本序列的真实长度，对真实标签序列进行截断\r\n",
    "            batch_tags = batch_tags.numpy().tolist()\r\n",
    "            real_paths = [tag_seq[:tag_len] for tag_seq, tag_len in zip(batch_tags, batch_lens)]\r\n",
    "\r\n",
    "            # 更新统计指标相关数据\r\n",
    "            metric.update(pred_paths=pred_paths, real_paths=real_paths)\r\n",
    "\r\n",
    "    # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metric.get_result()\r\n",
    "    #format_print(result)\r\n",
    "    metric.format_print(result)\r\n",
    "\r\n",
    "    return result \r\n",
    "\r\n",
    "\r\n",
    "from evaluating import Metrics\r\n",
    "from score import SeqEntityScore\r\n",
    "def evaluate_detail(model,test_loader):\r\n",
    "    # 定义统计评估指标的类\r\n",
    "    metric_ent = SeqEntityScore(clue_id2tag)\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    test_tag_lists = []\r\n",
    "    pred_tag_lists = []\r\n",
    "    with paddle.no_grad():\r\n",
    "        for step, batch in enumerate(test_loader):\r\n",
    "            # 获取数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            \r\n",
    "            # 前向计算，得出发射分数\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "\r\n",
    "            # 根据发射分数，利用CRF进行解码\r\n",
    "            pred_paths = model.decoder(features) #(N,seqlen)\r\n",
    "\r\n",
    "            # 将这些预测的标签序列进行id2tag，即转换为相应的标签\r\n",
    "            pred_paths = [[clue_id2tag[int(tag_id)] for tag_id in tag_seq] for tag_seq in pred_paths]\r\n",
    "            \r\n",
    "            # 根据文本序列的真实长度，对真实标签序列进行截断\r\n",
    "            batch_tags = batch_tags.numpy().tolist()\r\n",
    "            real_paths = [tag_seq[:tag_len] for tag_seq, tag_len in zip(batch_tags, batch_lens)]\r\n",
    "\r\n",
    "            # 更新统计指标相关数据\r\n",
    "            metric_ent.update(pred_paths=pred_paths, real_paths=real_paths)\r\n",
    "\r\n",
    "            test_tag_lists.extend([clue_id2tag[i] for real_path in real_paths for i in real_path  ] )\r\n",
    "            pred_tag_lists.extend([pred_path for pred_path in pred_paths])\r\n",
    "    \r\n",
    "    # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metric_ent.get_result()\r\n",
    "    metric_ent.report_scores(result)\r\n",
    "    \r\n",
    "    metrics = Metrics(test_tag_lists, pred_tag_lists, remove_O=False)\r\n",
    "    metrics.report_scores()\r\n",
    "    metrics.report_confusion_matrix()\r\n",
    "\r\n",
    "    return result \r\n",
    "\r\n",
    "\r\n",
    "train(model_withouCRF, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型保存的名称\r\n",
    "model_name = \"withoutCRF_model\"\r\n",
    "# 保存模型\r\n",
    "paddle.save(model_withouCRF.state_dict(), \"{}.pdparams\".format(model_name))\r\n",
    "paddle.save(optimizer.state_dict(), \"{}.optparams\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load\r\n",
    "layer_state_dict = paddle.load(\"withoutCRF_model.pdparams\")\r\n",
    "opt_state_dict = paddle.load(\"withoutCRF_model.optparams\")\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "model_withouCRF = NERModel_withoutCRF(vocab_size=vocab_size, embedding_size=embedding_size,\r\n",
    "                     hidden_size=hidden_size,label2id=clue_tag2id, n_layers=n_layers, drop_p=dropout_rate)\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.99,\r\n",
    "                                  parameters=model_withouCRF.parameters())\r\n",
    "                                \r\n",
    "model_withouCRF.set_state_dict(layer_state_dict)\r\n",
    "optimizer.set_state_dict(opt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_detail(model_withouCRF, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "metric = SeqEntityScore(clue_id2tag)\r\n",
    "def infer(model, text, word2id,tag2id):\r\n",
    "    model.eval()\r\n",
    "    # 数据处理\r\n",
    "    tokens = [word2id.get(w) for w in list(text)]\r\n",
    "    tokens_len = len(tokens)\r\n",
    "\r\n",
    "    # 构造输入模型的数据\r\n",
    "    tokens = paddle.to_tensor(tokens, dtype=\"int64\").unsqueeze(0)\r\n",
    "    tokens_mask = paddle.to_tensor([1] * tokens_len, dtype=\"int64\").unsqueeze(0)\r\n",
    "    tokens_len = paddle.to_tensor(tokens_len, dtype=\"int64\")\r\n",
    "\r\n",
    "    # 计算发射分数\r\n",
    "    features = model.forward_loss(tokens, tokens_mask, tokens_len)\r\n",
    "\r\n",
    "    # 根据发射分数进行解码\r\n",
    "    _, pred_paths = model.decoder(features)\r\n",
    "\r\n",
    "    print(pred_paths[0])\r\n",
    "    print([tag2id.get(int(x)) for x in pred_paths[0] ])\r\n",
    "    print(text)\r\n",
    "    # 解析路径中的实体\r\n",
    "    entities = SeqEntityScore(tag2id).get_entities_bios(pred_paths[0])\r\n",
    "    print(entities)\r\n",
    "    for entity in entities:\r\n",
    "        entity_type, start, end = entity\r\n",
    "        print(f\"{text[start:end+1]} : {entity_type}\")\r\n",
    "\r\n",
    "\r\n",
    "text=\"今年1月中国光大银行沈阳银行致电，称由于他的信用记录良好，银行可把普通信用卡免费升级为白金卡\"\r\n",
    "infer(model_withouCRF, text, clue_word2id, clue_id2tag)\r\n",
    "    \r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "# !ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "# !ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "# import sys \n",
    "# sys.path.append('//home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  BI-LSTM+CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddlenlp.layers.crf  as crf\r\n",
    "from score import SeqEntityScore\r\n",
    "from evaluating import Metrics\r\n",
    "\r\n",
    "def load_data(path):\r\n",
    "    # 加载数据集\r\n",
    "    def load_dataset(mode=\"train\"):\r\n",
    "        assert mode in [\"train\", \"dev\", \"test\"]\r\n",
    "        data_path = os.path.join(path, mode+\".json\")\r\n",
    "        examples = []\r\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\r\n",
    "            for idx, line in enumerate(f):\r\n",
    "                # 保存该行样本整理后的数据\r\n",
    "                example = {}\r\n",
    "                line = json.loads(line.strip())\r\n",
    "                text = line[\"text\"]\r\n",
    "                tag_entities = line.get(\"label\", None)\r\n",
    "                words = list(text)\r\n",
    "                tags = [\"0\"] * len(words)\r\n",
    "                if tag_entities is not None:\r\n",
    "                    for tag_name, tag_value in tag_entities.items():\r\n",
    "                        for entity_name, entity_index in tag_value.items():\r\n",
    "                            for start_index, end_index in entity_index:\r\n",
    "                                assert \"\".join(words[start_index:end_index+1]) == entity_name\r\n",
    "                                if start_index == end_index:\r\n",
    "                                    tags[start_index] = \"S-\" + tag_name\r\n",
    "                                else:\r\n",
    "                                    tags[start_index] = \"B-\" + tag_name\r\n",
    "                                    tags[start_index + 1:end_index + 1] = [\"I-\" + tag_name] * (len(entity_name) - 1)\r\n",
    "                example[\"text\"] = \" \".join(words)\r\n",
    "                example[\"tag\"] = \" \".join(tags)\r\n",
    "                examples.append(example)\r\n",
    "\r\n",
    "        return examples\r\n",
    "    \r\n",
    "    # 加载词典\r\n",
    "    def load_dict(dict_name):\r\n",
    "        assert dict_name in [\"tag\", \"vocab\"]\r\n",
    "        data_path = os.path.join(path, dict_name+\".dict\")\r\n",
    "        data_dict = {}\r\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\r\n",
    "            lines = [item.strip().split(\"\\t\") for item in f.readlines()]\r\n",
    "            data_dict = dict([(item[1], int(item[0])) for item in lines])\r\n",
    "\r\n",
    "        return data_dict \r\n",
    "\r\n",
    "    train_data = load_dataset(mode=\"train\")\r\n",
    "    test_data = load_dataset(mode=\"dev\")\r\n",
    "\r\n",
    "    word2id = load_dict(dict_name=\"vocab\")\r\n",
    "    tag2id = load_dict(dict_name=\"tag\")\r\n",
    "\r\n",
    "    return train_data, test_data, word2id, tag2id   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag2id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_180/10097652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtag2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tag2id' is not defined"
     ]
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conver_word_to_id(data, word2id, tag2id):\r\n",
    "        examples = []\r\n",
    "        for example in data:\r\n",
    "            text = example['text']\r\n",
    "            tokens = [word2id.get(w, word2id[\"[UNK]\"]) for w in text.split(\" \")]\r\n",
    "            text_real_len = len(tokens)\r\n",
    "            tag = example['tag']\r\n",
    "            tag_ids = [tag2id.get(t, tag2id[\"O\"]) for t in tag.split(\" \")]\r\n",
    "            examples.append((tokens, tag_ids, text_real_len))\r\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DatasetLoader(object):\r\n",
    "    def __init__(self, data, batch_size, shuffle=False, sort=True, drop_last=False):\r\n",
    "        self.examples = data\r\n",
    "        self.shuffle = shuffle\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.sort = sort\r\n",
    "        self.drop_last = drop_last\r\n",
    "        self.split_data()\r\n",
    "    \r\n",
    "    # 将数据分成mini-batch\r\n",
    "    def split_data(self):\r\n",
    "        # 根据样本的长度对数据进行排序\r\n",
    "        if self.sort:\r\n",
    "            self.examples = sorted(self.examples, key=lambda x: x[2], reverse=True)\r\n",
    "        if self.shuffle:\r\n",
    "            indices = list(range(len(self.examples)))\r\n",
    "            random.shuffle(indices)\r\n",
    "            self.examples = [self.examples[i] for i in indices]\r\n",
    "        self.batch_data = [self.examples[i:i + self.batch_size] for i in range(0, len(self.examples), self.batch_size)]\r\n",
    "        if self.drop_last and len(self.batch_data[-1]) < batch_size:\r\n",
    "            self.batch_data = self.batch_data[:-1]\r\n",
    "\r\n",
    "    def padding_sequence(self, batch, batch_size, mask=None):\r\n",
    "        # 固定batch中样本的长度，将短的样本padding到最长样本的长度\r\n",
    "        tokens_list, tags_list, lens = batch\r\n",
    "        max_len = max(lens)\r\n",
    "        batch_size = len(lens)\r\n",
    "        batch_token = paddle.full(shape=[batch_size, max_len], fill_value=0, dtype=\"int64\")\r\n",
    "        batch_tag = paddle.full(shape=[batch_size, max_len], fill_value=0, dtype=\"int64\")\r\n",
    "        batch_mask = paddle.full(shape=[batch_size, max_len], fill_value=0, dtype=\"int64\")\r\n",
    "        for i in range(batch_size):\r\n",
    "            batch_token[i, :lens[i]] = paddle.to_tensor(tokens_list[i], dtype=\"int64\")\r\n",
    "            batch_tag[i, :lens[i]] = paddle.to_tensor(tags_list[i], dtype=\"int64\")\r\n",
    "\r\n",
    "            if mask:\r\n",
    "                batch_mask[i, :lens[i]] = paddle.to_tensor([1] * lens[i], dtype=\"int64\") \r\n",
    "        if mask:\r\n",
    "            return batch_token, batch_tag, batch_mask\r\n",
    "\r\n",
    "        return batch_token, batch_tag\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.batch_data)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        # 每次调用该函数，则返回一个batch\r\n",
    "        # batch中每条样本包含三部分数据：text，tag, len\r\n",
    "        batch = self.batch_data[index]\r\n",
    "        batch_size = len(batch)\r\n",
    "        # batch变成三个list，依次是text，tag, len\r\n",
    "        batch = list(zip(*batch))\r\n",
    "        batch_ids, batch_tags, batch_mask = self.padding_sequence(batch, batch_size, mask=True)\r\n",
    "        batch_lens = paddle.to_tensor(batch[2])\r\n",
    "\r\n",
    "        return (batch_ids, batch_tags, batch_mask, batch_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NERModel(paddle.nn.Layer):\r\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, label2id, n_layers=2, drop_p=0.1):\r\n",
    "        super(NERModel, self).__init__()\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.label2id = label2id\r\n",
    "\r\n",
    "        self.embedding = paddle.nn.Embedding(vocab_size, embedding_size)\r\n",
    "        self.bilstm = paddle.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, direction=\"bidirectional\", num_layers=n_layers, dropout=drop_p)\r\n",
    "        self.layer_norm = paddle.nn.LayerNorm(hidden_size*2)\r\n",
    "        self.dropout_emb = paddle.nn.Dropout(p=drop_p)\r\n",
    "        # 在CRF的具体实现时，会引入2个辅助性标签<START> 和 <STOP>\r\n",
    "        self.classifier = paddle.nn.Linear(hidden_size*2, len(label2id)+2) # add START and STOP tag\r\n",
    "\r\n",
    "        # 将标签数量传入crf中，生成crf实例，这里需要注意一下n_labels不包含START和STOP标签\r\n",
    "        self.crf = crf.LinearChainCrf(len(label2id), crf_lr=0.001, with_start_stop_tag=True)\r\n",
    "        self.crf_loss = crf.LinearChainCrfLoss(self.crf)\r\n",
    "        self.viterbi_decoder = crf.ViterbiDecoder(self.crf.transitions)\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, input_ids, input_mask):\r\n",
    "        # 该前向计算将会输出bilstm最后一层的序列隐状态\r\n",
    "        # input_ids: [batch_size, seq_len]\r\n",
    "        # embs: [batch_size, seq_len, embedding_size]\r\n",
    "        embs = self.embedding(input_ids)\r\n",
    "        embs = self.dropout_emb(embs)\r\n",
    "        embs = embs * paddle.to_tensor(input_mask, dtype=\"float32\").unsqueeze(2)\r\n",
    "        # last_layer_hiddens: [batch_size, seq_len, hidden_size]\r\n",
    "        last_layer_hiddens, _ = self.bilstm(embs)\r\n",
    "        last_layer_hiddens = self.layer_norm(last_layer_hiddens)\r\n",
    "        # features: [batch_size, seq_len, n_labels]\r\n",
    "        features = self.classifier(last_layer_hiddens)\r\n",
    "\r\n",
    "        return features\r\n",
    "\r\n",
    "    def forward_loss(self, input_ids, input_mask, input_lens, input_tags=None):\r\n",
    "        # input_ids: [batch_size, seq_len]\r\n",
    "        # input_mask: [batch_size, seq_len]\r\n",
    "        # input_lens: [batch_size] Tensor\r\n",
    "        # input_tags: [batch_size, seq_len]\r\n",
    "\r\n",
    "        # features: [batch_size, seq_len, n_labels]\r\n",
    "        features = self.forward(input_ids, input_mask)\r\n",
    "\r\n",
    "        if input_tags is not None:\r\n",
    "            return features, self.crf_loss(features, input_lens, \"\", input_tags)\r\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addTags_2(tag2id):  \r\n",
    "    tag2id['<Start>'] = len(tag2id)\r\n",
    "    tag2id['<End>'] = len(tag2id)\r\n",
    "    return tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1118 22:18:20.714541   143 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W1118 22:18:20.760452   143 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\r\n",
    "root_path = \"./ner_clue_data/\"\r\n",
    "train_data, test_data, word2id, tag2id = load_data(root_path)\r\n",
    "train_set = conver_word_to_id(train_data, word2id, tag2id)\r\n",
    "test_set = conver_word_to_id(test_data, word2id, tag2id)\r\n",
    "\r\n",
    "addTags_2(tag2id)\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "n_epochs = 50\r\n",
    "batch_size = 32\r\n",
    "vocab_size = len(word2id.keys())\r\n",
    "embedding_size = 128\r\n",
    "hidden_size = 384\r\n",
    "n_layers = 2\r\n",
    "dropout_rate = 0.1\r\n",
    "learning_rate = 0.001\r\n",
    "\r\n",
    "# 生成data_loader，方便按照batch取数据\r\n",
    "train_loader = DatasetLoader(train_set, batch_size, shuffle=False, sort=True)\r\n",
    "test_loader = DatasetLoader(test_set, batch_size, shuffle=False, sort=False)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# 检测是否可以使用GPU，如果可以优先使用GPU\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "ner_model = NERModel(vocab_size=vocab_size, embedding_size=embedding_size,\r\n",
    "                     hidden_size=hidden_size,label2id=tag2id, n_layers=n_layers, drop_p=dropout_rate)\r\n",
    "\r\n",
    "# 指定优化器\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.99,\r\n",
    "                                  parameters=ner_model.parameters())\r\n",
    "\r\n",
    "# 反转tag2id, 得到id2tag字典\r\n",
    "id2tag = dict([items[1], items[0]] for items in tag2id.items())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-address',\n",
       " 2: 'B-book',\n",
       " 3: 'B-company',\n",
       " 4: 'B-game',\n",
       " 5: 'B-government',\n",
       " 6: 'B-movie',\n",
       " 7: 'B-name',\n",
       " 8: 'B-organization',\n",
       " 9: 'B-position',\n",
       " 10: 'B-scene',\n",
       " 11: 'I-address',\n",
       " 12: 'I-book',\n",
       " 13: 'I-company',\n",
       " 14: 'I-game',\n",
       " 15: 'I-government',\n",
       " 16: 'I-movie',\n",
       " 17: 'I-name',\n",
       " 18: 'I-organization',\n",
       " 19: 'I-position',\n",
       " 20: 'I-scene',\n",
       " 21: 'S-address',\n",
       " 22: 'S-book',\n",
       " 23: 'S-company',\n",
       " 24: 'S-game',\n",
       " 25: 'S-government',\n",
       " 26: 'S-movie',\n",
       " 27: 'S-name',\n",
       " 28: 'S-organization',\n",
       " 29: 'S-position',\n",
       " 30: 'S-scene',\n",
       " 31: '<Start>',\n",
       " 32: '<End>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "epoch: 1, step: 0, loss: 254.2734832763672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-18 22:18:45,893] [ WARNING] - Compatibility Warning: The params of LinearChainCrfLoss.forward has been modified. The third param is `labels`, and the fourth is not necessary. Please update the usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 20, loss: 66.33065795898438\n",
      "epoch: 1, step: 40, loss: 63.17569351196289\n",
      "epoch: 1, step: 60, loss: 42.73284149169922\n",
      "epoch: 1, step: 80, loss: 37.45672607421875\n",
      "epoch: 1, step: 100, loss: 31.633586883544922\n",
      "epoch: 1, step: 120, loss: 35.52635192871094\n",
      "epoch: 1, step: 140, loss: 33.965660095214844\n",
      "epoch: 1, step: 160, loss: 25.99199676513672\n",
      "epoch: 1, step: 180, loss: 24.506038665771484\n",
      "epoch: 1, step: 200, loss: 21.9235782623291\n",
      "epoch: 1, step: 220, loss: 27.952251434326172\n",
      "epoch: 1, step: 240, loss: 21.93344497680664\n",
      "epoch: 1, step: 260, loss: 14.469120979309082\n",
      "epoch: 1, step: 280, loss: 13.816789627075195\n",
      "epoch: 1, step: 300, loss: 16.045164108276367\n",
      "epoch: 1, step: 320, loss: 8.633285522460938\n",
      "Total: Precision: 0.3315 - Recall: 0.3831 - F1: 0.3554\n",
      "Epoch 2/50\n",
      "epoch: 2, step: 0, loss: 32.469356536865234\n",
      "epoch: 2, step: 20, loss: 22.231422424316406\n",
      "epoch: 2, step: 40, loss: 24.642513275146484\n",
      "epoch: 2, step: 60, loss: 21.22760009765625\n",
      "epoch: 2, step: 80, loss: 17.258953094482422\n",
      "epoch: 2, step: 100, loss: 17.124679565429688\n",
      "epoch: 2, step: 120, loss: 16.070903778076172\n",
      "epoch: 2, step: 140, loss: 19.858829498291016\n",
      "epoch: 2, step: 160, loss: 14.755038261413574\n",
      "epoch: 2, step: 180, loss: 16.502782821655273\n",
      "epoch: 2, step: 200, loss: 15.395919799804688\n",
      "epoch: 2, step: 220, loss: 20.298805236816406\n",
      "epoch: 2, step: 240, loss: 15.045316696166992\n",
      "epoch: 2, step: 260, loss: 12.415022850036621\n",
      "epoch: 2, step: 280, loss: 10.878524780273438\n",
      "epoch: 2, step: 300, loss: 11.39984130859375\n",
      "epoch: 2, step: 320, loss: 5.087685585021973\n",
      "Total: Precision: 0.394 - Recall: 0.4932 - F1: 0.4381\n",
      "Epoch 3/50\n",
      "epoch: 3, step: 0, loss: 24.989185333251953\n",
      "epoch: 3, step: 20, loss: 16.607620239257812\n",
      "epoch: 3, step: 40, loss: 19.834259033203125\n",
      "epoch: 3, step: 60, loss: 14.662425994873047\n",
      "epoch: 3, step: 80, loss: 14.756758689880371\n",
      "epoch: 3, step: 100, loss: 13.196492195129395\n",
      "epoch: 3, step: 120, loss: 12.991604804992676\n",
      "epoch: 3, step: 140, loss: 16.544147491455078\n",
      "epoch: 3, step: 160, loss: 11.680961608886719\n",
      "epoch: 3, step: 180, loss: 13.831124305725098\n",
      "epoch: 3, step: 200, loss: 12.425390243530273\n",
      "epoch: 3, step: 220, loss: 16.656999588012695\n",
      "epoch: 3, step: 240, loss: 11.526817321777344\n",
      "epoch: 3, step: 260, loss: 9.970816612243652\n",
      "epoch: 3, step: 280, loss: 9.088500022888184\n",
      "epoch: 3, step: 300, loss: 9.246098518371582\n",
      "epoch: 3, step: 320, loss: 3.873194932937622\n",
      "Total: Precision: 0.4886 - Recall: 0.5648 - F1: 0.5239\n",
      "Epoch 4/50\n",
      "epoch: 4, step: 0, loss: 18.36235237121582\n",
      "epoch: 4, step: 20, loss: 14.659011840820312\n",
      "epoch: 4, step: 40, loss: 17.73230743408203\n",
      "epoch: 4, step: 60, loss: 12.315874099731445\n",
      "epoch: 4, step: 80, loss: 11.998008728027344\n",
      "epoch: 4, step: 100, loss: 11.017391204833984\n",
      "epoch: 4, step: 120, loss: 11.111757278442383\n",
      "epoch: 4, step: 140, loss: 15.289624214172363\n",
      "epoch: 4, step: 160, loss: 9.99622917175293\n",
      "epoch: 4, step: 180, loss: 10.197269439697266\n",
      "epoch: 4, step: 200, loss: 11.423458099365234\n",
      "epoch: 4, step: 220, loss: 12.748764991760254\n",
      "epoch: 4, step: 240, loss: 9.79171085357666\n",
      "epoch: 4, step: 260, loss: 8.7554931640625\n",
      "epoch: 4, step: 280, loss: 7.801881790161133\n",
      "epoch: 4, step: 300, loss: 7.401926040649414\n",
      "epoch: 4, step: 320, loss: 3.1266400814056396\n",
      "Total: Precision: 0.4882 - Recall: 0.5866 - F1: 0.5329\n",
      "Epoch 5/50\n",
      "epoch: 5, step: 0, loss: 14.834318161010742\n",
      "epoch: 5, step: 20, loss: 12.37564754486084\n",
      "epoch: 5, step: 40, loss: 14.552499771118164\n",
      "epoch: 5, step: 60, loss: 10.008683204650879\n",
      "epoch: 5, step: 80, loss: 10.98831844329834\n",
      "epoch: 5, step: 100, loss: 10.029580116271973\n",
      "epoch: 5, step: 120, loss: 8.425592422485352\n",
      "epoch: 5, step: 140, loss: 15.103227615356445\n",
      "epoch: 5, step: 160, loss: 8.67039680480957\n",
      "epoch: 5, step: 180, loss: 8.093497276306152\n",
      "epoch: 5, step: 200, loss: 8.847797393798828\n",
      "epoch: 5, step: 220, loss: 13.698677062988281\n",
      "epoch: 5, step: 240, loss: 8.006494522094727\n",
      "epoch: 5, step: 260, loss: 7.324631214141846\n",
      "epoch: 5, step: 280, loss: 7.146229267120361\n",
      "epoch: 5, step: 300, loss: 6.788156032562256\n",
      "epoch: 5, step: 320, loss: 3.0913732051849365\n",
      "Total: Precision: 0.503 - Recall: 0.597 - F1: 0.546\n",
      "Epoch 6/50\n",
      "epoch: 6, step: 0, loss: 13.691671371459961\n",
      "epoch: 6, step: 20, loss: 11.542552947998047\n",
      "epoch: 6, step: 40, loss: 13.104384422302246\n",
      "epoch: 6, step: 60, loss: 9.749565124511719\n",
      "epoch: 6, step: 80, loss: 9.98740005493164\n",
      "epoch: 6, step: 100, loss: 7.147849082946777\n",
      "epoch: 6, step: 120, loss: 8.10857105255127\n",
      "epoch: 6, step: 140, loss: 12.666193962097168\n",
      "epoch: 6, step: 160, loss: 7.254612922668457\n",
      "epoch: 6, step: 180, loss: 7.323581695556641\n",
      "epoch: 6, step: 200, loss: 7.788909912109375\n",
      "epoch: 6, step: 220, loss: 10.802103996276855\n",
      "epoch: 6, step: 240, loss: 6.826520919799805\n",
      "epoch: 6, step: 260, loss: 6.911868095397949\n",
      "epoch: 6, step: 280, loss: 6.7784953117370605\n",
      "epoch: 6, step: 300, loss: 6.790946960449219\n",
      "epoch: 6, step: 320, loss: 3.0383245944976807\n",
      "Total: Precision: 0.5198 - Recall: 0.599 - F1: 0.5566\n",
      "Epoch 7/50\n",
      "epoch: 7, step: 0, loss: 11.523726463317871\n",
      "epoch: 7, step: 20, loss: 9.816350936889648\n",
      "epoch: 7, step: 40, loss: 12.731760025024414\n",
      "epoch: 7, step: 60, loss: 9.349817276000977\n",
      "epoch: 7, step: 80, loss: 9.901480674743652\n",
      "epoch: 7, step: 100, loss: 6.231657981872559\n",
      "epoch: 7, step: 120, loss: 6.656298637390137\n",
      "epoch: 7, step: 140, loss: 10.644617080688477\n",
      "epoch: 7, step: 160, loss: 5.994210243225098\n",
      "epoch: 7, step: 180, loss: 6.639110565185547\n",
      "epoch: 7, step: 200, loss: 7.114819526672363\n",
      "epoch: 7, step: 220, loss: 9.348424911499023\n",
      "epoch: 7, step: 240, loss: 6.879717826843262\n",
      "epoch: 7, step: 260, loss: 5.342308521270752\n",
      "epoch: 7, step: 280, loss: 6.03629732131958\n",
      "epoch: 7, step: 300, loss: 6.2624831199646\n",
      "epoch: 7, step: 320, loss: 2.7897095680236816\n",
      "Total: Precision: 0.5084 - Recall: 0.612 - F1: 0.5554\n",
      "Epoch 8/50\n",
      "epoch: 8, step: 0, loss: 11.976877212524414\n",
      "epoch: 8, step: 20, loss: 9.819138526916504\n",
      "epoch: 8, step: 40, loss: 11.197888374328613\n",
      "epoch: 8, step: 60, loss: 7.92359733581543\n",
      "epoch: 8, step: 80, loss: 8.004344940185547\n",
      "epoch: 8, step: 100, loss: 7.278132438659668\n",
      "epoch: 8, step: 120, loss: 5.837214469909668\n",
      "epoch: 8, step: 140, loss: 11.2378568649292\n",
      "epoch: 8, step: 160, loss: 5.407892227172852\n",
      "epoch: 8, step: 180, loss: 5.750561714172363\n",
      "epoch: 8, step: 200, loss: 4.792716979980469\n",
      "epoch: 8, step: 220, loss: 7.795110702514648\n",
      "epoch: 8, step: 240, loss: 4.998410224914551\n",
      "epoch: 8, step: 260, loss: 4.6269683837890625\n",
      "epoch: 8, step: 280, loss: 5.9837327003479\n",
      "epoch: 8, step: 300, loss: 6.06650447845459\n",
      "epoch: 8, step: 320, loss: 2.2042295932769775\n",
      "Total: Precision: 0.529 - Recall: 0.6123 - F1: 0.5676\n",
      "Epoch 9/50\n",
      "epoch: 9, step: 0, loss: 8.851995468139648\n",
      "epoch: 9, step: 20, loss: 8.188276290893555\n",
      "epoch: 9, step: 40, loss: 8.847041130065918\n",
      "epoch: 9, step: 60, loss: 7.530745506286621\n",
      "epoch: 9, step: 80, loss: 7.820213317871094\n",
      "epoch: 9, step: 100, loss: 5.474308967590332\n",
      "epoch: 9, step: 120, loss: 7.045432090759277\n",
      "epoch: 9, step: 140, loss: 11.427229881286621\n",
      "epoch: 9, step: 160, loss: 4.498723983764648\n",
      "epoch: 9, step: 180, loss: 6.1143598556518555\n",
      "epoch: 9, step: 200, loss: 6.810874938964844\n",
      "epoch: 9, step: 220, loss: 8.311152458190918\n",
      "epoch: 9, step: 240, loss: 4.105628967285156\n",
      "epoch: 9, step: 260, loss: 3.527707099914551\n",
      "epoch: 9, step: 280, loss: 4.839111804962158\n",
      "epoch: 9, step: 300, loss: 5.0495476722717285\n",
      "epoch: 9, step: 320, loss: 2.2718594074249268\n",
      "Total: Precision: 0.5299 - Recall: 0.6165 - F1: 0.57\n",
      "Epoch 10/50\n",
      "epoch: 10, step: 0, loss: 8.38056755065918\n",
      "epoch: 10, step: 20, loss: 7.05672550201416\n",
      "epoch: 10, step: 40, loss: 9.487517356872559\n",
      "epoch: 10, step: 60, loss: 7.729641914367676\n",
      "epoch: 10, step: 80, loss: 7.238848686218262\n",
      "epoch: 10, step: 100, loss: 6.353172302246094\n",
      "epoch: 10, step: 120, loss: 5.552700042724609\n",
      "epoch: 10, step: 140, loss: 8.74894905090332\n",
      "epoch: 10, step: 160, loss: 3.7547035217285156\n",
      "epoch: 10, step: 180, loss: 5.460674285888672\n",
      "epoch: 10, step: 200, loss: 4.541053771972656\n",
      "epoch: 10, step: 220, loss: 5.830292701721191\n",
      "epoch: 10, step: 240, loss: 3.3681650161743164\n",
      "epoch: 10, step: 260, loss: 4.209799766540527\n",
      "epoch: 10, step: 280, loss: 4.951676845550537\n",
      "epoch: 10, step: 300, loss: 4.2434258460998535\n",
      "epoch: 10, step: 320, loss: 1.685777187347412\n",
      "Total: Precision: 0.5605 - Recall: 0.626 - F1: 0.5914\n",
      "Epoch 11/50\n",
      "epoch: 11, step: 0, loss: 5.748203277587891\n",
      "epoch: 11, step: 20, loss: 5.904863357543945\n",
      "epoch: 11, step: 40, loss: 8.810516357421875\n",
      "epoch: 11, step: 60, loss: 4.647150993347168\n",
      "epoch: 11, step: 80, loss: 5.886452674865723\n",
      "epoch: 11, step: 100, loss: 5.814335823059082\n",
      "epoch: 11, step: 120, loss: 4.340973854064941\n",
      "epoch: 11, step: 140, loss: 11.470253944396973\n",
      "epoch: 11, step: 160, loss: 3.2893476486206055\n",
      "epoch: 11, step: 180, loss: 3.314028739929199\n",
      "epoch: 11, step: 200, loss: 4.574324607849121\n",
      "epoch: 11, step: 220, loss: 5.530364990234375\n",
      "epoch: 11, step: 240, loss: 3.069211006164551\n",
      "epoch: 11, step: 260, loss: 2.2801504135131836\n",
      "epoch: 11, step: 280, loss: 2.5436854362487793\n",
      "epoch: 11, step: 300, loss: 3.8104300498962402\n",
      "epoch: 11, step: 320, loss: 1.3357653617858887\n",
      "Total: Precision: 0.5644 - Recall: 0.6247 - F1: 0.593\n",
      "Epoch 12/50\n",
      "epoch: 12, step: 0, loss: 4.841509819030762\n",
      "epoch: 12, step: 20, loss: 5.329313278198242\n",
      "epoch: 12, step: 40, loss: 5.801824569702148\n",
      "epoch: 12, step: 60, loss: 4.293952941894531\n",
      "epoch: 12, step: 80, loss: 4.656101226806641\n",
      "epoch: 12, step: 100, loss: 3.8419065475463867\n",
      "epoch: 12, step: 120, loss: 3.4907188415527344\n",
      "epoch: 12, step: 140, loss: 5.321237564086914\n",
      "epoch: 12, step: 160, loss: 3.218149185180664\n",
      "epoch: 12, step: 180, loss: 2.9503793716430664\n",
      "epoch: 12, step: 200, loss: 2.0047197341918945\n",
      "epoch: 12, step: 220, loss: 4.540353775024414\n",
      "epoch: 12, step: 240, loss: 2.245774269104004\n",
      "epoch: 12, step: 260, loss: 2.2208003997802734\n",
      "epoch: 12, step: 280, loss: 2.1573486328125\n",
      "epoch: 12, step: 300, loss: 3.1594061851501465\n",
      "epoch: 12, step: 320, loss: 0.9094448089599609\n",
      "Total: Precision: 0.5485 - Recall: 0.6204 - F1: 0.5823\n",
      "Epoch 13/50\n",
      "epoch: 13, step: 0, loss: 4.282780647277832\n",
      "epoch: 13, step: 20, loss: 3.7995967864990234\n",
      "epoch: 13, step: 40, loss: 5.724769592285156\n",
      "epoch: 13, step: 60, loss: 2.5539369583129883\n",
      "epoch: 13, step: 80, loss: 3.681110382080078\n",
      "epoch: 13, step: 100, loss: 3.3544797897338867\n",
      "epoch: 13, step: 120, loss: 5.233065605163574\n",
      "epoch: 13, step: 140, loss: 4.752565383911133\n",
      "epoch: 13, step: 160, loss: 2.202059745788574\n",
      "epoch: 13, step: 180, loss: 3.744487762451172\n",
      "epoch: 13, step: 200, loss: 1.7343673706054688\n",
      "epoch: 13, step: 220, loss: 4.8753767013549805\n",
      "epoch: 13, step: 240, loss: 2.9071712493896484\n",
      "epoch: 13, step: 260, loss: 1.6531639099121094\n",
      "epoch: 13, step: 280, loss: 2.664741039276123\n",
      "epoch: 13, step: 300, loss: 3.1832118034362793\n",
      "epoch: 13, step: 320, loss: 1.4251422882080078\n",
      "Total: Precision: 0.5731 - Recall: 0.6204 - F1: 0.5958\n",
      "Epoch 14/50\n",
      "epoch: 14, step: 0, loss: 3.867772102355957\n",
      "epoch: 14, step: 20, loss: 3.3329696655273438\n",
      "epoch: 14, step: 40, loss: 5.117441177368164\n",
      "epoch: 14, step: 60, loss: 3.6298294067382812\n",
      "epoch: 14, step: 80, loss: 4.263816833496094\n",
      "epoch: 14, step: 100, loss: 2.7403879165649414\n",
      "epoch: 14, step: 120, loss: 2.821077346801758\n",
      "epoch: 14, step: 140, loss: 5.25978946685791\n",
      "epoch: 14, step: 160, loss: 2.708925247192383\n",
      "epoch: 14, step: 180, loss: 1.2707748413085938\n",
      "epoch: 14, step: 200, loss: 2.5497350692749023\n",
      "epoch: 14, step: 220, loss: 4.724709510803223\n",
      "epoch: 14, step: 240, loss: 2.11160945892334\n",
      "epoch: 14, step: 260, loss: 0.9981365203857422\n",
      "epoch: 14, step: 280, loss: 2.635812759399414\n",
      "epoch: 14, step: 300, loss: 1.6110968589782715\n",
      "epoch: 14, step: 320, loss: 0.689211368560791\n",
      "Total: Precision: 0.5729 - Recall: 0.6156 - F1: 0.5934\n",
      "Epoch 15/50\n",
      "epoch: 15, step: 0, loss: 2.7133941650390625\n",
      "epoch: 15, step: 20, loss: 2.5596179962158203\n",
      "epoch: 15, step: 40, loss: 2.8409042358398438\n",
      "epoch: 15, step: 60, loss: 1.9818973541259766\n",
      "epoch: 15, step: 80, loss: 2.845582962036133\n",
      "epoch: 15, step: 100, loss: 2.0046682357788086\n",
      "epoch: 15, step: 120, loss: 2.4716873168945312\n",
      "epoch: 15, step: 140, loss: 2.611429214477539\n",
      "epoch: 15, step: 160, loss: 1.574437141418457\n",
      "epoch: 15, step: 180, loss: 2.7303619384765625\n",
      "epoch: 15, step: 200, loss: 1.3909292221069336\n",
      "epoch: 15, step: 220, loss: 3.34110164642334\n",
      "epoch: 15, step: 240, loss: 2.491168975830078\n",
      "epoch: 15, step: 260, loss: 0.9027957916259766\n",
      "epoch: 15, step: 280, loss: 2.287881851196289\n",
      "epoch: 15, step: 300, loss: 1.5402846336364746\n",
      "epoch: 15, step: 320, loss: 0.7057881355285645\n",
      "Total: Precision: 0.5904 - Recall: 0.6263 - F1: 0.6078\n",
      "Epoch 16/50\n",
      "epoch: 16, step: 0, loss: 2.878549575805664\n",
      "epoch: 16, step: 20, loss: 2.099689483642578\n",
      "epoch: 16, step: 40, loss: 3.5352096557617188\n",
      "epoch: 16, step: 60, loss: 2.1705551147460938\n",
      "epoch: 16, step: 80, loss: 2.1349029541015625\n",
      "epoch: 16, step: 100, loss: 2.108475685119629\n",
      "epoch: 16, step: 120, loss: 2.246959686279297\n",
      "epoch: 16, step: 140, loss: 1.6344213485717773\n",
      "epoch: 16, step: 160, loss: 1.3975944519042969\n",
      "epoch: 16, step: 180, loss: 1.5904932022094727\n",
      "epoch: 16, step: 200, loss: 1.1669397354125977\n",
      "epoch: 16, step: 220, loss: 2.875657081604004\n",
      "epoch: 16, step: 240, loss: 1.6087141036987305\n",
      "epoch: 16, step: 260, loss: 1.00152587890625\n",
      "epoch: 16, step: 280, loss: 0.9584159851074219\n",
      "epoch: 16, step: 300, loss: 2.3075122833251953\n",
      "epoch: 16, step: 320, loss: 0.5548532009124756\n",
      "Total: Precision: 0.5691 - Recall: 0.6263 - F1: 0.5963\n",
      "Epoch 17/50\n",
      "epoch: 17, step: 0, loss: 1.9655094146728516\n",
      "epoch: 17, step: 20, loss: 2.4259681701660156\n",
      "epoch: 17, step: 40, loss: 1.4111080169677734\n",
      "epoch: 17, step: 60, loss: 1.7368412017822266\n",
      "epoch: 17, step: 80, loss: 0.7954788208007812\n",
      "epoch: 17, step: 100, loss: 1.6589202880859375\n",
      "epoch: 17, step: 120, loss: 1.6451301574707031\n",
      "epoch: 17, step: 140, loss: 1.8264284133911133\n",
      "epoch: 17, step: 160, loss: 1.4405155181884766\n",
      "epoch: 17, step: 180, loss: 1.4303340911865234\n",
      "epoch: 17, step: 200, loss: 1.4385061264038086\n",
      "epoch: 17, step: 220, loss: 1.7588434219360352\n",
      "epoch: 17, step: 240, loss: 2.051285743713379\n",
      "epoch: 17, step: 260, loss: 1.0041313171386719\n",
      "epoch: 17, step: 280, loss: 1.1486530303955078\n",
      "epoch: 17, step: 300, loss: 1.7855448722839355\n",
      "epoch: 17, step: 320, loss: 0.5787639617919922\n",
      "Total: Precision: 0.5686 - Recall: 0.6445 - F1: 0.6042\n",
      "Epoch 18/50\n",
      "epoch: 18, step: 0, loss: 2.487058639526367\n",
      "epoch: 18, step: 20, loss: 1.7201957702636719\n",
      "epoch: 18, step: 40, loss: 1.3884925842285156\n",
      "epoch: 18, step: 60, loss: 0.8295745849609375\n",
      "epoch: 18, step: 80, loss: 1.3011341094970703\n",
      "epoch: 18, step: 100, loss: 1.1290416717529297\n",
      "epoch: 18, step: 120, loss: 1.9592857360839844\n",
      "epoch: 18, step: 140, loss: 1.4828310012817383\n",
      "epoch: 18, step: 160, loss: 2.0673389434814453\n",
      "epoch: 18, step: 180, loss: 1.8980903625488281\n",
      "epoch: 18, step: 200, loss: 0.7684049606323242\n",
      "epoch: 18, step: 220, loss: 2.8445348739624023\n",
      "epoch: 18, step: 240, loss: 0.9332895278930664\n",
      "epoch: 18, step: 260, loss: 0.46695613861083984\n",
      "epoch: 18, step: 280, loss: 0.7084360122680664\n",
      "epoch: 18, step: 300, loss: 0.5860114097595215\n",
      "epoch: 18, step: 320, loss: 0.419741153717041\n",
      "Total: Precision: 0.5893 - Recall: 0.6305 - F1: 0.6092\n",
      "Epoch 19/50\n",
      "epoch: 19, step: 0, loss: 1.2997493743896484\n",
      "epoch: 19, step: 20, loss: 1.1496467590332031\n",
      "epoch: 19, step: 40, loss: 0.5699081420898438\n",
      "epoch: 19, step: 60, loss: 1.0477542877197266\n",
      "epoch: 19, step: 80, loss: 1.7597427368164062\n",
      "epoch: 19, step: 100, loss: 2.683765411376953\n",
      "epoch: 19, step: 120, loss: 0.8835639953613281\n",
      "epoch: 19, step: 140, loss: 1.3001689910888672\n",
      "epoch: 19, step: 160, loss: 0.7368698120117188\n",
      "epoch: 19, step: 180, loss: 1.9695396423339844\n",
      "epoch: 19, step: 200, loss: 0.8439912796020508\n",
      "epoch: 19, step: 220, loss: 1.5071067810058594\n",
      "epoch: 19, step: 240, loss: 0.6203861236572266\n",
      "epoch: 19, step: 260, loss: 0.5936450958251953\n",
      "epoch: 19, step: 280, loss: 0.5197858810424805\n",
      "epoch: 19, step: 300, loss: 0.9954261779785156\n",
      "epoch: 19, step: 320, loss: 0.753995418548584\n",
      "Total: Precision: 0.586 - Recall: 0.6335 - F1: 0.6088\n",
      "Epoch 20/50\n",
      "epoch: 20, step: 0, loss: 1.6681098937988281\n",
      "epoch: 20, step: 20, loss: 1.2091712951660156\n",
      "epoch: 20, step: 40, loss: 0.6887474060058594\n",
      "epoch: 20, step: 60, loss: 1.9533710479736328\n",
      "epoch: 20, step: 80, loss: 0.8956623077392578\n",
      "epoch: 20, step: 100, loss: 1.3920402526855469\n",
      "epoch: 20, step: 120, loss: 2.21734619140625\n",
      "epoch: 20, step: 140, loss: 0.7694320678710938\n",
      "epoch: 20, step: 160, loss: 1.1572589874267578\n",
      "epoch: 20, step: 180, loss: 1.8003044128417969\n",
      "epoch: 20, step: 200, loss: 0.9258012771606445\n",
      "epoch: 20, step: 220, loss: 1.4412097930908203\n",
      "epoch: 20, step: 240, loss: 0.5767583847045898\n",
      "epoch: 20, step: 260, loss: 0.4638967514038086\n",
      "epoch: 20, step: 280, loss: 0.7030544281005859\n",
      "epoch: 20, step: 300, loss: 0.9428234100341797\n",
      "epoch: 20, step: 320, loss: 0.2442183494567871\n",
      "Total: Precision: 0.5964 - Recall: 0.6133 - F1: 0.6047\n",
      "Epoch 21/50\n",
      "epoch: 21, step: 0, loss: 1.0153141021728516\n",
      "epoch: 21, step: 20, loss: 0.6746311187744141\n",
      "epoch: 21, step: 40, loss: 0.5095062255859375\n",
      "epoch: 21, step: 60, loss: 1.0719432830810547\n",
      "epoch: 21, step: 80, loss: 0.9384956359863281\n",
      "epoch: 21, step: 100, loss: 1.6894702911376953\n",
      "epoch: 21, step: 120, loss: 1.1432666778564453\n",
      "epoch: 21, step: 140, loss: 0.8441286087036133\n",
      "epoch: 21, step: 160, loss: 0.9635753631591797\n",
      "epoch: 21, step: 180, loss: 1.7296333312988281\n",
      "epoch: 21, step: 200, loss: 0.9359321594238281\n",
      "epoch: 21, step: 220, loss: 1.4780502319335938\n",
      "epoch: 21, step: 240, loss: 0.4888277053833008\n",
      "epoch: 21, step: 260, loss: 0.2794160842895508\n",
      "epoch: 21, step: 280, loss: 1.0957317352294922\n",
      "epoch: 21, step: 300, loss: 0.6938323974609375\n",
      "epoch: 21, step: 320, loss: 0.36359620094299316\n",
      "Total: Precision: 0.5967 - Recall: 0.6204 - F1: 0.6084\n",
      "Epoch 22/50\n",
      "epoch: 22, step: 0, loss: 1.2503795623779297\n",
      "epoch: 22, step: 20, loss: 0.9094276428222656\n",
      "epoch: 22, step: 40, loss: 0.6721096038818359\n",
      "epoch: 22, step: 60, loss: 0.6411762237548828\n",
      "epoch: 22, step: 80, loss: 1.22100830078125\n",
      "epoch: 22, step: 100, loss: 0.6991024017333984\n",
      "epoch: 22, step: 120, loss: 0.47655487060546875\n",
      "epoch: 22, step: 140, loss: 0.5042314529418945\n",
      "epoch: 22, step: 160, loss: 0.5513286590576172\n",
      "epoch: 22, step: 180, loss: 0.9511394500732422\n",
      "epoch: 22, step: 200, loss: 1.3030624389648438\n",
      "epoch: 22, step: 220, loss: 0.9502677917480469\n",
      "epoch: 22, step: 240, loss: 0.5899639129638672\n",
      "epoch: 22, step: 260, loss: 0.28717708587646484\n",
      "epoch: 22, step: 280, loss: 1.2478361129760742\n",
      "epoch: 22, step: 300, loss: 0.34842538833618164\n",
      "epoch: 22, step: 320, loss: 0.9946885108947754\n",
      "Total: Precision: 0.6133 - Recall: 0.6263 - F1: 0.6197\n",
      "Epoch 23/50\n",
      "epoch: 23, step: 0, loss: 1.7097434997558594\n",
      "epoch: 23, step: 20, loss: 0.5880107879638672\n",
      "epoch: 23, step: 40, loss: 0.6669387817382812\n",
      "epoch: 23, step: 60, loss: 0.3403606414794922\n",
      "epoch: 23, step: 80, loss: 0.6538486480712891\n",
      "epoch: 23, step: 100, loss: 1.304779052734375\n",
      "epoch: 23, step: 120, loss: 0.5458774566650391\n",
      "epoch: 23, step: 140, loss: 0.7037801742553711\n",
      "epoch: 23, step: 160, loss: 0.4281578063964844\n",
      "epoch: 23, step: 180, loss: 1.6759986877441406\n",
      "epoch: 23, step: 200, loss: 0.4255352020263672\n",
      "epoch: 23, step: 220, loss: 0.7942800521850586\n",
      "epoch: 23, step: 240, loss: 0.6368017196655273\n",
      "epoch: 23, step: 260, loss: 0.5796165466308594\n",
      "epoch: 23, step: 280, loss: 0.6626272201538086\n",
      "epoch: 23, step: 300, loss: 0.5728802680969238\n",
      "epoch: 23, step: 320, loss: 0.4193534851074219\n",
      "Total: Precision: 0.6006 - Recall: 0.6257 - F1: 0.6129\n",
      "Epoch 24/50\n",
      "epoch: 24, step: 0, loss: 0.8999423980712891\n",
      "epoch: 24, step: 20, loss: 0.5332717895507812\n",
      "epoch: 24, step: 40, loss: 0.3505744934082031\n",
      "epoch: 24, step: 60, loss: 0.5033817291259766\n",
      "epoch: 24, step: 80, loss: 1.520181655883789\n",
      "epoch: 24, step: 100, loss: 1.2759132385253906\n",
      "epoch: 24, step: 120, loss: 1.150075912475586\n",
      "epoch: 24, step: 140, loss: 0.9707794189453125\n",
      "epoch: 24, step: 160, loss: 0.4648323059082031\n",
      "epoch: 24, step: 180, loss: 0.45502662658691406\n",
      "epoch: 24, step: 200, loss: 0.9114561080932617\n",
      "epoch: 24, step: 220, loss: 0.8448419570922852\n",
      "epoch: 24, step: 240, loss: 0.2869873046875\n",
      "epoch: 24, step: 260, loss: 0.34119224548339844\n",
      "epoch: 24, step: 280, loss: 0.55364990234375\n",
      "epoch: 24, step: 300, loss: 0.44534778594970703\n",
      "epoch: 24, step: 320, loss: 0.26927757263183594\n",
      "Total: Precision: 0.6014 - Recall: 0.6227 - F1: 0.6119\n",
      "Epoch 25/50\n",
      "epoch: 25, step: 0, loss: 0.9497432708740234\n",
      "epoch: 25, step: 20, loss: 0.6413440704345703\n",
      "epoch: 25, step: 40, loss: 0.8080463409423828\n",
      "epoch: 25, step: 60, loss: 0.5607147216796875\n",
      "epoch: 25, step: 80, loss: 0.5160789489746094\n",
      "epoch: 25, step: 100, loss: 1.1729698181152344\n",
      "epoch: 25, step: 120, loss: 0.6793098449707031\n",
      "epoch: 25, step: 140, loss: 1.137685775756836\n",
      "epoch: 25, step: 160, loss: 1.5297794342041016\n",
      "epoch: 25, step: 180, loss: 0.9148654937744141\n",
      "epoch: 25, step: 200, loss: 0.4447917938232422\n",
      "epoch: 25, step: 220, loss: 0.7491254806518555\n",
      "epoch: 25, step: 240, loss: 0.7104606628417969\n",
      "epoch: 25, step: 260, loss: 0.5132408142089844\n",
      "epoch: 25, step: 280, loss: 0.6596479415893555\n",
      "epoch: 25, step: 300, loss: 0.5469889640808105\n",
      "epoch: 25, step: 320, loss: 2.0239272117614746\n",
      "Total: Precision: 0.6091 - Recall: 0.6243 - F1: 0.6166\n",
      "Epoch 26/50\n",
      "epoch: 26, step: 0, loss: 1.0430965423583984\n",
      "epoch: 26, step: 20, loss: 0.8761062622070312\n",
      "epoch: 26, step: 40, loss: 0.7261638641357422\n",
      "epoch: 26, step: 60, loss: 0.27596473693847656\n",
      "epoch: 26, step: 80, loss: 0.5298213958740234\n",
      "epoch: 26, step: 100, loss: 0.6595973968505859\n",
      "epoch: 26, step: 120, loss: 0.9098701477050781\n",
      "epoch: 26, step: 140, loss: 0.7215499877929688\n",
      "epoch: 26, step: 160, loss: 0.2315654754638672\n",
      "epoch: 26, step: 180, loss: 0.2680015563964844\n",
      "epoch: 26, step: 200, loss: 0.7653751373291016\n",
      "epoch: 26, step: 220, loss: 1.0316953659057617\n",
      "epoch: 26, step: 240, loss: 0.701385498046875\n",
      "epoch: 26, step: 260, loss: 0.4438037872314453\n",
      "epoch: 26, step: 280, loss: 0.18060779571533203\n",
      "epoch: 26, step: 300, loss: 0.20938682556152344\n",
      "epoch: 26, step: 320, loss: 0.1352071762084961\n",
      "Total: Precision: 0.6065 - Recall: 0.6413 - F1: 0.6234\n",
      "Epoch 27/50\n",
      "epoch: 27, step: 0, loss: 2.398134231567383\n",
      "epoch: 27, step: 20, loss: 0.38751792907714844\n",
      "epoch: 27, step: 40, loss: 2.59075927734375\n",
      "epoch: 27, step: 60, loss: 0.2551002502441406\n",
      "epoch: 27, step: 80, loss: 1.3760700225830078\n",
      "epoch: 27, step: 100, loss: 0.4815959930419922\n",
      "epoch: 27, step: 120, loss: 1.0926551818847656\n",
      "epoch: 27, step: 140, loss: 0.5589256286621094\n",
      "epoch: 27, step: 160, loss: 0.2998390197753906\n",
      "epoch: 27, step: 180, loss: 0.3259239196777344\n",
      "epoch: 27, step: 200, loss: 1.0223102569580078\n",
      "epoch: 27, step: 220, loss: 0.7810039520263672\n",
      "epoch: 27, step: 240, loss: 0.506749153137207\n",
      "epoch: 27, step: 260, loss: 0.2322683334350586\n",
      "epoch: 27, step: 280, loss: 0.20770263671875\n",
      "epoch: 27, step: 300, loss: 0.270294189453125\n",
      "epoch: 27, step: 320, loss: 0.8554816246032715\n",
      "Total: Precision: 0.6134 - Recall: 0.6445 - F1: 0.6286\n",
      "Epoch 28/50\n",
      "epoch: 28, step: 0, loss: 0.5225162506103516\n",
      "epoch: 28, step: 20, loss: 0.6729087829589844\n",
      "epoch: 28, step: 40, loss: 1.939535140991211\n",
      "epoch: 28, step: 60, loss: 0.45455169677734375\n",
      "epoch: 28, step: 80, loss: 0.7045345306396484\n",
      "epoch: 28, step: 100, loss: 0.7253704071044922\n",
      "epoch: 28, step: 120, loss: 0.4084339141845703\n",
      "epoch: 28, step: 140, loss: 0.24985694885253906\n",
      "epoch: 28, step: 160, loss: 1.0009803771972656\n",
      "epoch: 28, step: 180, loss: 0.21451377868652344\n",
      "epoch: 28, step: 200, loss: 0.08486557006835938\n",
      "epoch: 28, step: 220, loss: 0.862645149230957\n",
      "epoch: 28, step: 240, loss: 0.29218482971191406\n",
      "epoch: 28, step: 260, loss: 0.7360525131225586\n",
      "epoch: 28, step: 280, loss: 0.29523658752441406\n",
      "epoch: 28, step: 300, loss: 0.14611101150512695\n",
      "epoch: 28, step: 320, loss: 0.14536762237548828\n",
      "Total: Precision: 0.5979 - Recall: 0.626 - F1: 0.6116\n",
      "Epoch 29/50\n",
      "epoch: 29, step: 0, loss: 0.7489833831787109\n",
      "epoch: 29, step: 20, loss: 0.5554027557373047\n",
      "epoch: 29, step: 40, loss: 0.5692272186279297\n",
      "epoch: 29, step: 60, loss: 0.14737510681152344\n",
      "epoch: 29, step: 80, loss: 0.4296302795410156\n",
      "epoch: 29, step: 100, loss: 0.9248790740966797\n",
      "epoch: 29, step: 120, loss: 0.26861572265625\n",
      "epoch: 29, step: 140, loss: 0.5513629913330078\n",
      "epoch: 29, step: 160, loss: 0.3213768005371094\n",
      "epoch: 29, step: 180, loss: 1.2917022705078125\n",
      "epoch: 29, step: 200, loss: 0.3574790954589844\n",
      "epoch: 29, step: 220, loss: 0.49153709411621094\n",
      "epoch: 29, step: 240, loss: 0.3562488555908203\n",
      "epoch: 29, step: 260, loss: 0.13492584228515625\n",
      "epoch: 29, step: 280, loss: 0.1426095962524414\n",
      "epoch: 29, step: 300, loss: 0.31227588653564453\n",
      "epoch: 29, step: 320, loss: 0.1350870132446289\n",
      "Total: Precision: 0.6107 - Recall: 0.6328 - F1: 0.6216\n",
      "Epoch 30/50\n",
      "epoch: 30, step: 0, loss: 1.0849380493164062\n",
      "epoch: 30, step: 20, loss: 0.4188976287841797\n",
      "epoch: 30, step: 40, loss: 0.8705215454101562\n",
      "epoch: 30, step: 60, loss: 0.6151599884033203\n",
      "epoch: 30, step: 80, loss: 0.7860546112060547\n",
      "epoch: 30, step: 100, loss: 0.6808643341064453\n",
      "epoch: 30, step: 120, loss: 0.14447021484375\n",
      "epoch: 30, step: 140, loss: 0.373321533203125\n",
      "epoch: 30, step: 160, loss: 0.34366416931152344\n",
      "epoch: 30, step: 180, loss: 0.7167491912841797\n",
      "epoch: 30, step: 200, loss: 0.19303321838378906\n",
      "epoch: 30, step: 220, loss: 0.6105976104736328\n",
      "epoch: 30, step: 240, loss: 0.1711435317993164\n",
      "epoch: 30, step: 260, loss: 0.12790679931640625\n",
      "epoch: 30, step: 280, loss: 0.16876506805419922\n",
      "epoch: 30, step: 300, loss: 0.2223224639892578\n",
      "epoch: 30, step: 320, loss: 0.3616456985473633\n",
      "Total: Precision: 0.6062 - Recall: 0.6169 - F1: 0.6115\n",
      "Epoch 31/50\n",
      "epoch: 31, step: 0, loss: 0.2670173645019531\n",
      "epoch: 31, step: 20, loss: 0.6315059661865234\n",
      "epoch: 31, step: 40, loss: 0.23287391662597656\n",
      "epoch: 31, step: 60, loss: 0.7013568878173828\n",
      "epoch: 31, step: 80, loss: 0.7101058959960938\n",
      "epoch: 31, step: 100, loss: 1.207956314086914\n",
      "epoch: 31, step: 120, loss: 0.24428558349609375\n",
      "epoch: 31, step: 140, loss: 0.6198768615722656\n",
      "epoch: 31, step: 160, loss: 0.4492759704589844\n",
      "epoch: 31, step: 180, loss: 0.2301483154296875\n",
      "epoch: 31, step: 200, loss: 0.6513347625732422\n",
      "epoch: 31, step: 220, loss: 0.29323768615722656\n",
      "epoch: 31, step: 240, loss: 0.2372570037841797\n",
      "epoch: 31, step: 260, loss: 0.18575668334960938\n",
      "epoch: 31, step: 280, loss: 0.6099815368652344\n",
      "epoch: 31, step: 300, loss: 0.19831085205078125\n",
      "epoch: 31, step: 320, loss: 0.2098073959350586\n",
      "Total: Precision: 0.612 - Recall: 0.6465 - F1: 0.6288\n",
      "Epoch 32/50\n",
      "epoch: 32, step: 0, loss: 0.7443447113037109\n",
      "epoch: 32, step: 20, loss: 0.4215221405029297\n",
      "epoch: 32, step: 40, loss: 0.2319355010986328\n",
      "epoch: 32, step: 60, loss: 0.3250560760498047\n",
      "epoch: 32, step: 80, loss: 0.8644561767578125\n",
      "epoch: 32, step: 100, loss: 0.7085819244384766\n",
      "epoch: 32, step: 120, loss: 0.3575630187988281\n",
      "epoch: 32, step: 140, loss: 0.1772327423095703\n",
      "epoch: 32, step: 160, loss: 0.21457481384277344\n",
      "epoch: 32, step: 180, loss: 0.36863136291503906\n",
      "epoch: 32, step: 200, loss: 0.07686805725097656\n",
      "epoch: 32, step: 220, loss: 0.3375682830810547\n",
      "epoch: 32, step: 240, loss: 0.24182415008544922\n",
      "epoch: 32, step: 260, loss: 0.11287498474121094\n",
      "epoch: 32, step: 280, loss: 0.3428773880004883\n",
      "epoch: 32, step: 300, loss: 0.5634236335754395\n",
      "epoch: 32, step: 320, loss: 0.13663578033447266\n",
      "Total: Precision: 0.5841 - Recall: 0.6465 - F1: 0.6137\n",
      "Epoch 33/50\n",
      "epoch: 33, step: 0, loss: 0.49814414978027344\n",
      "epoch: 33, step: 20, loss: 0.353851318359375\n",
      "epoch: 33, step: 40, loss: 0.2683906555175781\n",
      "epoch: 33, step: 60, loss: 0.2935199737548828\n",
      "epoch: 33, step: 80, loss: 0.36981773376464844\n",
      "epoch: 33, step: 100, loss: 0.4949073791503906\n",
      "epoch: 33, step: 120, loss: 1.025909423828125\n",
      "epoch: 33, step: 140, loss: 0.20476722717285156\n",
      "epoch: 33, step: 160, loss: 0.7938289642333984\n",
      "epoch: 33, step: 180, loss: 0.6628055572509766\n",
      "epoch: 33, step: 200, loss: 0.11228561401367188\n",
      "epoch: 33, step: 220, loss: 0.4634590148925781\n",
      "epoch: 33, step: 240, loss: 0.1363534927368164\n",
      "epoch: 33, step: 260, loss: 0.044586181640625\n",
      "epoch: 33, step: 280, loss: 0.21811676025390625\n",
      "epoch: 33, step: 300, loss: 0.2287750244140625\n",
      "epoch: 33, step: 320, loss: 0.14517450332641602\n",
      "Total: Precision: 0.6153 - Recall: 0.6341 - F1: 0.6246\n",
      "Epoch 34/50\n",
      "epoch: 34, step: 0, loss: 0.3025665283203125\n",
      "epoch: 34, step: 20, loss: 0.17234039306640625\n",
      "epoch: 34, step: 40, loss: 0.7236824035644531\n",
      "epoch: 34, step: 60, loss: 0.3970050811767578\n",
      "epoch: 34, step: 80, loss: 0.5652065277099609\n",
      "epoch: 34, step: 100, loss: 0.39630699157714844\n",
      "epoch: 34, step: 120, loss: 0.3471641540527344\n",
      "epoch: 34, step: 140, loss: 0.26832008361816406\n",
      "epoch: 34, step: 160, loss: 0.635498046875\n",
      "epoch: 34, step: 180, loss: 0.3430328369140625\n",
      "epoch: 34, step: 200, loss: 0.3211250305175781\n",
      "epoch: 34, step: 220, loss: 0.42349815368652344\n",
      "epoch: 34, step: 240, loss: 0.44898319244384766\n",
      "epoch: 34, step: 260, loss: 0.45934391021728516\n",
      "epoch: 34, step: 280, loss: 0.2534370422363281\n",
      "epoch: 34, step: 300, loss: 0.11267280578613281\n",
      "epoch: 34, step: 320, loss: 0.35730552673339844\n",
      "Total: Precision: 0.6223 - Recall: 0.6462 - F1: 0.634\n",
      "Epoch 35/50\n",
      "epoch: 35, step: 0, loss: 0.4085674285888672\n",
      "epoch: 35, step: 20, loss: 0.8253269195556641\n",
      "epoch: 35, step: 40, loss: 0.25134849548339844\n",
      "epoch: 35, step: 60, loss: 0.4423637390136719\n",
      "epoch: 35, step: 80, loss: 0.5857391357421875\n",
      "epoch: 35, step: 100, loss: 0.7745304107666016\n",
      "epoch: 35, step: 120, loss: 0.7403430938720703\n",
      "epoch: 35, step: 140, loss: 0.2257862091064453\n",
      "epoch: 35, step: 160, loss: 0.2522754669189453\n",
      "epoch: 35, step: 180, loss: 1.2027473449707031\n",
      "epoch: 35, step: 200, loss: 0.15757369995117188\n",
      "epoch: 35, step: 220, loss: 0.4747943878173828\n",
      "epoch: 35, step: 240, loss: 0.42584228515625\n",
      "epoch: 35, step: 260, loss: 0.34322071075439453\n",
      "epoch: 35, step: 280, loss: 0.17640209197998047\n",
      "epoch: 35, step: 300, loss: 0.33861732482910156\n",
      "epoch: 35, step: 320, loss: 0.13652706146240234\n",
      "Total: Precision: 0.6165 - Recall: 0.6406 - F1: 0.6284\n",
      "Epoch 36/50\n",
      "epoch: 36, step: 0, loss: 0.2596435546875\n",
      "epoch: 36, step: 20, loss: 0.4594917297363281\n",
      "epoch: 36, step: 40, loss: 0.3136444091796875\n",
      "epoch: 36, step: 60, loss: 0.4170265197753906\n",
      "epoch: 36, step: 80, loss: 0.10296058654785156\n",
      "epoch: 36, step: 100, loss: 0.5382499694824219\n",
      "epoch: 36, step: 120, loss: 0.4271049499511719\n",
      "epoch: 36, step: 140, loss: 0.16879844665527344\n",
      "epoch: 36, step: 160, loss: 0.20444869995117188\n",
      "epoch: 36, step: 180, loss: 0.3012104034423828\n",
      "epoch: 36, step: 200, loss: 0.31964111328125\n",
      "epoch: 36, step: 220, loss: 1.0211963653564453\n",
      "epoch: 36, step: 240, loss: 0.3194255828857422\n",
      "epoch: 36, step: 260, loss: 0.1258687973022461\n",
      "epoch: 36, step: 280, loss: 0.26864051818847656\n",
      "epoch: 36, step: 300, loss: 0.18420886993408203\n",
      "epoch: 36, step: 320, loss: 0.09576940536499023\n",
      "Total: Precision: 0.6271 - Recall: 0.6361 - F1: 0.6315\n",
      "Epoch 37/50\n",
      "epoch: 37, step: 0, loss: 0.3921375274658203\n",
      "epoch: 37, step: 20, loss: 0.49843406677246094\n",
      "epoch: 37, step: 40, loss: 0.5972003936767578\n",
      "epoch: 37, step: 60, loss: 1.111032485961914\n",
      "epoch: 37, step: 80, loss: 0.1785717010498047\n",
      "epoch: 37, step: 100, loss: 0.4832725524902344\n",
      "epoch: 37, step: 120, loss: 0.178253173828125\n",
      "epoch: 37, step: 140, loss: 0.13496971130371094\n",
      "epoch: 37, step: 160, loss: 0.15601730346679688\n",
      "epoch: 37, step: 180, loss: 0.3936138153076172\n",
      "epoch: 37, step: 200, loss: 1.1048450469970703\n",
      "epoch: 37, step: 220, loss: 0.6157512664794922\n",
      "epoch: 37, step: 240, loss: 3.225088119506836\n",
      "epoch: 37, step: 260, loss: 0.49691104888916016\n",
      "epoch: 37, step: 280, loss: 0.4120302200317383\n",
      "epoch: 37, step: 300, loss: 0.14552593231201172\n",
      "epoch: 37, step: 320, loss: 0.10370635986328125\n",
      "Total: Precision: 0.6109 - Recall: 0.6436 - F1: 0.6268\n",
      "Epoch 38/50\n",
      "epoch: 38, step: 0, loss: 0.4533119201660156\n",
      "epoch: 38, step: 20, loss: 0.5095920562744141\n",
      "epoch: 38, step: 40, loss: 0.2912330627441406\n",
      "epoch: 38, step: 60, loss: 0.4424858093261719\n",
      "epoch: 38, step: 80, loss: 0.6044864654541016\n",
      "epoch: 38, step: 100, loss: 0.24251556396484375\n",
      "epoch: 38, step: 120, loss: 0.28885459899902344\n",
      "epoch: 38, step: 140, loss: 0.5340671539306641\n",
      "epoch: 38, step: 160, loss: 0.5944404602050781\n",
      "epoch: 38, step: 180, loss: 0.2566089630126953\n",
      "epoch: 38, step: 200, loss: 0.3734397888183594\n",
      "epoch: 38, step: 220, loss: 0.4359550476074219\n",
      "epoch: 38, step: 240, loss: 0.4393482208251953\n",
      "epoch: 38, step: 260, loss: 0.07825851440429688\n",
      "epoch: 38, step: 280, loss: 0.24775123596191406\n",
      "epoch: 38, step: 300, loss: 0.44487953186035156\n",
      "epoch: 38, step: 320, loss: 0.11249685287475586\n",
      "Total: Precision: 0.6182 - Recall: 0.6357 - F1: 0.6269\n",
      "Epoch 39/50\n",
      "epoch: 39, step: 0, loss: 0.4896125793457031\n",
      "epoch: 39, step: 20, loss: 0.3789329528808594\n",
      "epoch: 39, step: 40, loss: 0.15744781494140625\n",
      "epoch: 39, step: 60, loss: 0.1894702911376953\n",
      "epoch: 39, step: 80, loss: 0.12705230712890625\n",
      "epoch: 39, step: 100, loss: 0.3213825225830078\n",
      "epoch: 39, step: 120, loss: 0.16304397583007812\n",
      "epoch: 39, step: 140, loss: 0.26137351989746094\n",
      "epoch: 39, step: 160, loss: 0.1961688995361328\n",
      "epoch: 39, step: 180, loss: 0.17696571350097656\n",
      "epoch: 39, step: 200, loss: 0.06755828857421875\n",
      "epoch: 39, step: 220, loss: 0.6297607421875\n",
      "epoch: 39, step: 240, loss: 0.07247734069824219\n",
      "epoch: 39, step: 260, loss: 0.1493072509765625\n",
      "epoch: 39, step: 280, loss: 0.10636138916015625\n",
      "epoch: 39, step: 300, loss: 0.6944408416748047\n",
      "epoch: 39, step: 320, loss: 0.26851463317871094\n",
      "Total: Precision: 0.6029 - Recall: 0.6449 - F1: 0.6232\n",
      "Epoch 40/50\n",
      "epoch: 40, step: 0, loss: 0.8162117004394531\n",
      "epoch: 40, step: 20, loss: 0.3028907775878906\n",
      "epoch: 40, step: 40, loss: 0.27515220642089844\n",
      "epoch: 40, step: 60, loss: 0.3535022735595703\n",
      "epoch: 40, step: 80, loss: 0.37821388244628906\n",
      "epoch: 40, step: 100, loss: 0.28484535217285156\n",
      "epoch: 40, step: 120, loss: 0.45157814025878906\n",
      "epoch: 40, step: 140, loss: 0.3021678924560547\n",
      "epoch: 40, step: 160, loss: 0.2340412139892578\n",
      "epoch: 40, step: 180, loss: 0.20355796813964844\n",
      "epoch: 40, step: 200, loss: 0.045993804931640625\n",
      "epoch: 40, step: 220, loss: 0.3367195129394531\n",
      "epoch: 40, step: 240, loss: 0.0842437744140625\n",
      "epoch: 40, step: 260, loss: 0.1703166961669922\n",
      "epoch: 40, step: 280, loss: 0.11268424987792969\n",
      "epoch: 40, step: 300, loss: 0.12969684600830078\n",
      "epoch: 40, step: 320, loss: 0.09282398223876953\n",
      "Total: Precision: 0.6193 - Recall: 0.6364 - F1: 0.6277\n",
      "Epoch 41/50\n",
      "epoch: 41, step: 0, loss: 0.7406845092773438\n",
      "epoch: 41, step: 20, loss: 0.09863662719726562\n",
      "epoch: 41, step: 40, loss: 0.4277000427246094\n",
      "epoch: 41, step: 60, loss: 0.08015060424804688\n",
      "epoch: 41, step: 80, loss: 0.3426704406738281\n",
      "epoch: 41, step: 100, loss: 0.2697315216064453\n",
      "epoch: 41, step: 120, loss: 0.10487174987792969\n",
      "epoch: 41, step: 140, loss: 0.2886486053466797\n",
      "epoch: 41, step: 160, loss: 0.1638965606689453\n",
      "epoch: 41, step: 180, loss: 0.15185546875\n",
      "epoch: 41, step: 200, loss: 0.09380531311035156\n",
      "epoch: 41, step: 220, loss: 0.6953296661376953\n",
      "epoch: 41, step: 240, loss: 0.06395339965820312\n",
      "epoch: 41, step: 260, loss: 0.07118988037109375\n",
      "epoch: 41, step: 280, loss: 0.16343116760253906\n",
      "epoch: 41, step: 300, loss: 0.23778820037841797\n",
      "epoch: 41, step: 320, loss: 0.15078353881835938\n",
      "Total: Precision: 0.6283 - Recall: 0.6344 - F1: 0.6314\n",
      "Epoch 42/50\n",
      "epoch: 42, step: 0, loss: 0.27814292907714844\n",
      "epoch: 42, step: 20, loss: 0.3253803253173828\n",
      "epoch: 42, step: 40, loss: 0.23931312561035156\n",
      "epoch: 42, step: 60, loss: 0.5033836364746094\n",
      "epoch: 42, step: 80, loss: 0.2128620147705078\n",
      "epoch: 42, step: 100, loss: 0.08977699279785156\n",
      "epoch: 42, step: 120, loss: 0.4948902130126953\n",
      "epoch: 42, step: 140, loss: 0.42237091064453125\n",
      "epoch: 42, step: 160, loss: 0.6143665313720703\n",
      "epoch: 42, step: 180, loss: 0.5059852600097656\n",
      "epoch: 42, step: 200, loss: 0.18427276611328125\n",
      "epoch: 42, step: 220, loss: 0.2249774932861328\n",
      "epoch: 42, step: 240, loss: 0.25022125244140625\n",
      "epoch: 42, step: 260, loss: 0.2620830535888672\n",
      "epoch: 42, step: 280, loss: 0.2258167266845703\n",
      "epoch: 42, step: 300, loss: 0.09612083435058594\n",
      "epoch: 42, step: 320, loss: 0.13967323303222656\n",
      "Total: Precision: 0.6239 - Recall: 0.6426 - F1: 0.6331\n",
      "Epoch 43/50\n",
      "epoch: 43, step: 0, loss: 0.09162712097167969\n",
      "epoch: 43, step: 20, loss: 0.21133804321289062\n",
      "epoch: 43, step: 40, loss: 0.3332481384277344\n",
      "epoch: 43, step: 60, loss: 0.35941314697265625\n",
      "epoch: 43, step: 80, loss: 0.16878318786621094\n",
      "epoch: 43, step: 100, loss: 0.09931564331054688\n",
      "epoch: 43, step: 120, loss: 0.3531303405761719\n",
      "epoch: 43, step: 140, loss: 0.5587348937988281\n",
      "epoch: 43, step: 160, loss: 0.167755126953125\n",
      "epoch: 43, step: 180, loss: 1.0237483978271484\n",
      "epoch: 43, step: 200, loss: 0.06386947631835938\n",
      "epoch: 43, step: 220, loss: 0.13536453247070312\n",
      "epoch: 43, step: 240, loss: 0.42701053619384766\n",
      "epoch: 43, step: 260, loss: 0.05129432678222656\n",
      "epoch: 43, step: 280, loss: 0.4104804992675781\n",
      "epoch: 43, step: 300, loss: 0.1059885025024414\n",
      "epoch: 43, step: 320, loss: 0.17247533798217773\n",
      "Total: Precision: 0.6054 - Recall: 0.6393 - F1: 0.6219\n",
      "Epoch 44/50\n",
      "epoch: 44, step: 0, loss: 0.5168399810791016\n",
      "epoch: 44, step: 20, loss: 0.10893630981445312\n",
      "epoch: 44, step: 40, loss: 0.16157150268554688\n",
      "epoch: 44, step: 60, loss: 0.1066131591796875\n",
      "epoch: 44, step: 80, loss: 0.11866188049316406\n",
      "epoch: 44, step: 100, loss: 0.13207626342773438\n",
      "epoch: 44, step: 120, loss: 0.40415382385253906\n",
      "epoch: 44, step: 140, loss: 0.4162635803222656\n",
      "epoch: 44, step: 160, loss: 0.06923675537109375\n",
      "epoch: 44, step: 180, loss: 0.47002601623535156\n",
      "epoch: 44, step: 200, loss: 0.19503021240234375\n",
      "epoch: 44, step: 220, loss: 0.8303890228271484\n",
      "epoch: 44, step: 240, loss: 0.2808055877685547\n",
      "epoch: 44, step: 260, loss: 0.09026813507080078\n",
      "epoch: 44, step: 280, loss: 0.09351730346679688\n",
      "epoch: 44, step: 300, loss: 0.18735599517822266\n",
      "epoch: 44, step: 320, loss: 0.11910486221313477\n",
      "Total: Precision: 0.6295 - Recall: 0.637 - F1: 0.6332\n",
      "Epoch 45/50\n",
      "epoch: 45, step: 0, loss: 0.14963531494140625\n",
      "epoch: 45, step: 20, loss: 0.22539710998535156\n",
      "epoch: 45, step: 40, loss: 0.13601303100585938\n",
      "epoch: 45, step: 60, loss: 0.10814094543457031\n",
      "epoch: 45, step: 80, loss: 0.16927528381347656\n",
      "epoch: 45, step: 100, loss: 0.3756427764892578\n",
      "epoch: 45, step: 120, loss: 0.3111686706542969\n",
      "epoch: 45, step: 140, loss: 0.6139888763427734\n",
      "epoch: 45, step: 160, loss: 0.25448036193847656\n",
      "epoch: 45, step: 180, loss: 0.4048957824707031\n",
      "epoch: 45, step: 200, loss: 0.6376609802246094\n",
      "epoch: 45, step: 220, loss: 0.15714454650878906\n",
      "epoch: 45, step: 240, loss: 0.19226455688476562\n",
      "epoch: 45, step: 260, loss: 0.2866325378417969\n",
      "epoch: 45, step: 280, loss: 0.10512256622314453\n",
      "epoch: 45, step: 300, loss: 0.15118026733398438\n",
      "epoch: 45, step: 320, loss: 0.1081857681274414\n",
      "Total: Precision: 0.625 - Recall: 0.6429 - F1: 0.6338\n",
      "Epoch 46/50\n",
      "epoch: 46, step: 0, loss: 0.1907329559326172\n",
      "epoch: 46, step: 20, loss: 0.2209930419921875\n",
      "epoch: 46, step: 40, loss: 0.1505413055419922\n",
      "epoch: 46, step: 60, loss: 0.3745307922363281\n",
      "epoch: 46, step: 80, loss: 0.42994117736816406\n",
      "epoch: 46, step: 100, loss: 0.6044826507568359\n",
      "epoch: 46, step: 120, loss: 0.12926864624023438\n",
      "epoch: 46, step: 140, loss: 0.24557113647460938\n",
      "epoch: 46, step: 160, loss: 0.5418968200683594\n",
      "epoch: 46, step: 180, loss: 0.1250133514404297\n",
      "epoch: 46, step: 200, loss: 0.09087181091308594\n",
      "epoch: 46, step: 220, loss: 0.3628826141357422\n",
      "epoch: 46, step: 240, loss: 1.1029033660888672\n",
      "epoch: 46, step: 260, loss: 0.19736099243164062\n",
      "epoch: 46, step: 280, loss: 4.3245954513549805\n",
      "epoch: 46, step: 300, loss: 0.1402454376220703\n",
      "epoch: 46, step: 320, loss: 2.6568126678466797\n",
      "Total: Precision: 0.6066 - Recall: 0.6416 - F1: 0.6236\n",
      "Epoch 47/50\n",
      "epoch: 47, step: 0, loss: 0.0721282958984375\n",
      "epoch: 47, step: 20, loss: 0.08581352233886719\n",
      "epoch: 47, step: 40, loss: 0.09239006042480469\n",
      "epoch: 47, step: 60, loss: 0.07523536682128906\n",
      "epoch: 47, step: 80, loss: 0.3344230651855469\n",
      "epoch: 47, step: 100, loss: 0.38409423828125\n",
      "epoch: 47, step: 120, loss: 0.4214458465576172\n",
      "epoch: 47, step: 140, loss: 0.5025405883789062\n",
      "epoch: 47, step: 160, loss: 0.7345676422119141\n",
      "epoch: 47, step: 180, loss: 1.0211448669433594\n",
      "epoch: 47, step: 200, loss: 0.13664627075195312\n",
      "epoch: 47, step: 220, loss: 0.3232402801513672\n",
      "epoch: 47, step: 240, loss: 0.12517356872558594\n",
      "epoch: 47, step: 260, loss: 0.053592681884765625\n",
      "epoch: 47, step: 280, loss: 0.19632434844970703\n",
      "epoch: 47, step: 300, loss: 0.08831596374511719\n",
      "epoch: 47, step: 320, loss: 0.0876922607421875\n",
      "Total: Precision: 0.6294 - Recall: 0.6445 - F1: 0.6369\n",
      "Epoch 48/50\n",
      "epoch: 48, step: 0, loss: 0.2796649932861328\n",
      "epoch: 48, step: 20, loss: 0.5712528228759766\n",
      "epoch: 48, step: 40, loss: 0.05513191223144531\n",
      "epoch: 48, step: 60, loss: 0.4534339904785156\n",
      "epoch: 48, step: 80, loss: 0.18787765502929688\n",
      "epoch: 48, step: 100, loss: 0.18407821655273438\n",
      "epoch: 48, step: 120, loss: 0.0786590576171875\n",
      "epoch: 48, step: 140, loss: 0.09338760375976562\n",
      "epoch: 48, step: 160, loss: 0.3985271453857422\n",
      "epoch: 48, step: 180, loss: 0.46373748779296875\n",
      "epoch: 48, step: 200, loss: 0.3334980010986328\n",
      "epoch: 48, step: 220, loss: 0.08841133117675781\n",
      "epoch: 48, step: 240, loss: 0.0294342041015625\n",
      "epoch: 48, step: 260, loss: 0.05707550048828125\n",
      "epoch: 48, step: 280, loss: 0.1767892837524414\n",
      "epoch: 48, step: 300, loss: 0.14072608947753906\n",
      "epoch: 48, step: 320, loss: 0.40435171127319336\n",
      "Total: Precision: 0.6246 - Recall: 0.6322 - F1: 0.6284\n",
      "Epoch 49/50\n",
      "epoch: 49, step: 0, loss: 0.2829303741455078\n",
      "epoch: 49, step: 20, loss: 0.5743484497070312\n",
      "epoch: 49, step: 40, loss: 0.29924964904785156\n",
      "epoch: 49, step: 60, loss: 0.11418533325195312\n",
      "epoch: 49, step: 80, loss: 0.6560859680175781\n",
      "epoch: 49, step: 100, loss: 0.5356655120849609\n",
      "epoch: 49, step: 120, loss: 0.18190574645996094\n",
      "epoch: 49, step: 140, loss: 0.1358184814453125\n",
      "epoch: 49, step: 160, loss: 0.34536170959472656\n",
      "epoch: 49, step: 180, loss: 0.12896728515625\n",
      "epoch: 49, step: 200, loss: 0.1085357666015625\n",
      "epoch: 49, step: 220, loss: 0.19570159912109375\n",
      "epoch: 49, step: 240, loss: 0.028459548950195312\n",
      "epoch: 49, step: 260, loss: 0.07764816284179688\n",
      "epoch: 49, step: 280, loss: 0.15871334075927734\n",
      "epoch: 49, step: 300, loss: 0.10560131072998047\n",
      "epoch: 49, step: 320, loss: 0.08817100524902344\n",
      "Total: Precision: 0.6565 - Recall: 0.6432 - F1: 0.6498\n",
      "Epoch 50/50\n",
      "epoch: 50, step: 0, loss: 0.1495361328125\n",
      "epoch: 50, step: 20, loss: 0.15891647338867188\n",
      "epoch: 50, step: 40, loss: 0.14905548095703125\n",
      "epoch: 50, step: 60, loss: 0.144683837890625\n",
      "epoch: 50, step: 80, loss: 0.3223228454589844\n",
      "epoch: 50, step: 100, loss: 0.1467761993408203\n",
      "epoch: 50, step: 120, loss: 0.2225818634033203\n",
      "epoch: 50, step: 140, loss: 0.2563934326171875\n",
      "epoch: 50, step: 160, loss: 0.30729103088378906\n",
      "epoch: 50, step: 180, loss: 0.17199325561523438\n",
      "epoch: 50, step: 200, loss: 0.07543563842773438\n",
      "epoch: 50, step: 220, loss: 0.16840171813964844\n",
      "epoch: 50, step: 240, loss: 0.18633079528808594\n",
      "epoch: 50, step: 260, loss: 0.02266693115234375\n",
      "epoch: 50, step: 280, loss: 0.09111881256103516\n",
      "epoch: 50, step: 300, loss: 0.2541818618774414\n",
      "epoch: 50, step: 320, loss: 0.24966144561767578\n",
      "Total: Precision: 0.6346 - Recall: 0.6491 - F1: 0.6418\n"
     ]
    }
   ],
   "source": [
    " \r\n",
    "\r\n",
    "# 模型评估\r\n",
    "def evaluate(model,test_loader):\r\n",
    "    # 定义统计评估指标的类\r\n",
    "    metric = SeqEntityScore(id2tag)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    with paddle.no_grad():\r\n",
    "        for step, batch in enumerate(test_loader):\r\n",
    "            # 获取数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            \r\n",
    "            # 前向计算，得出发射分数\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "\r\n",
    "            # 根据发射分数，利用CRF进行解码\r\n",
    "            scores, pred_paths = model.viterbi_decoder(features, batch_lens)\r\n",
    "\r\n",
    "            # 将这些预测的标签序列进行id2tag，即转换为相应的标签\r\n",
    "            pred_paths = [[id2tag[int(tag_id)] for tag_id in tag_seq] for tag_seq in pred_paths]\r\n",
    "            \r\n",
    "            # 根据文本序列的真实长度，对真实标签序列进行截断\r\n",
    "            batch_tags = batch_tags.numpy().tolist()\r\n",
    "            real_paths = [tag_seq[:tag_len] for tag_seq, tag_len in zip(batch_tags, batch_lens)]\r\n",
    "\r\n",
    "            # 更新统计指标相关数据\r\n",
    "            metric.update(pred_paths=pred_paths, real_paths=real_paths)\r\n",
    "\r\n",
    "    # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metric.get_result()\r\n",
    "    #format_print(result)\r\n",
    "    metric.format_print(result)\r\n",
    "\r\n",
    "    return result \r\n",
    "\r\n",
    "\r\n",
    "# 模型训练\r\n",
    "def train(model, train_loader):\r\n",
    "\r\n",
    "    for epoch in range(1, 1 + n_epochs):\r\n",
    "        model.train()\r\n",
    "        print(f\"Epoch {epoch}/{n_epochs}\")\r\n",
    "        for step, batch in enumerate(train_loader):\r\n",
    "            # 获取batch中的数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            # 执行模型的前向计算，并计算出损失\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "            loss = paddle.mean(loss)\r\n",
    "            \r\n",
    "            # 梯度计算和反向参数更新\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_gradients()\r\n",
    "\r\n",
    "            # 训练过程中打印信息\r\n",
    "            if step % 20 ==0:\r\n",
    "                print(f\"epoch: {epoch}, step: {step}, loss: {loss.numpy()[0]}\")\r\n",
    "        \r\n",
    "        # 模型评估\r\n",
    "        evaluate(model, test_loader)\r\n",
    "\r\n",
    "\r\n",
    "train(ner_model, train_loader)\r\n",
    "# 模型保存的名称\r\n",
    "model_name = \"ReRunNer_model\"\r\n",
    "# 保存模型\r\n",
    "paddle.save(ner_model.state_dict(), \"{}.pdparams\".format(model_name))\r\n",
    "paddle.save(optimizer.state_dict(), \"{}.optparams\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load():\r\n",
    "    # load\r\n",
    "    RERUNNER_layer_state_dict = paddle.load(\"ReRunNer_model.pdparams\")\r\n",
    "    RERUNNER_opt_state_dict = paddle.load(\"ReRunNer_model.optparams\")\r\n",
    "\r\n",
    "    # 实例化模型\r\n",
    "    RERUNNER__model = NERModel(vocab_size=vocab_size, embedding_size=embedding_size,\r\n",
    "                        hidden_size=hidden_size,label2id=tag2id, n_layers=n_layers, drop_p=dropout_rate)\r\n",
    "\r\n",
    "    # 指定优化器\r\n",
    "    optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.99,\r\n",
    "                                    parameters=RERUNNER__model.parameters())\r\n",
    "                                    \r\n",
    "    RERUNNER__model.set_state_dict(RERUNNER_layer_state_dict)\r\n",
    "    optimizer.set_state_dict(RERUNNER_opt_state_dict)\r\n",
    "    return RERUNNER__model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BiLSTMCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Precision: 0.6346 - Recall: 0.6491 - F1: 0.6418\n",
      "                 precision    recall  f1-score   support\n",
      "           name     0.7085    0.6796    0.6937       465\n",
      "        address     0.4297    0.4424    0.4359       373\n",
      "   organization     0.6548    0.7493    0.6989       367\n",
      "           game     0.7021    0.8068    0.7508       295\n",
      "          scene     0.4922    0.4545    0.4726       209\n",
      "           book     0.6410    0.6494    0.6452       154\n",
      "        company     0.6136    0.6429    0.6279       378\n",
      "       position     0.7210    0.6744    0.6969       433\n",
      "     government     0.6552    0.6923    0.6732       247\n",
      "          movie     0.6972    0.6556    0.6758       151\n",
      "      avg/total     0.6346    0.6491    0.6418      3072\n",
      "                 precision    recall  f1-score   support\n",
      "         B-name     0.0381    0.0366    0.0373       465\n",
      "         I-name     0.0446    0.0421    0.0433      1021\n",
      "              O     0.5530    0.8032    0.6550     36747\n",
      "      B-address     0.0130    0.0134    0.0132       373\n",
      "      I-address     0.0453    0.0451    0.0452      1329\n",
      " B-organization     0.0190    0.0218    0.0203       367\n",
      " I-organization     0.0221    0.0230    0.0225      1087\n",
      "         B-game     0.0472    0.0542    0.0505       295\n",
      "         I-game     0.0646    0.0720    0.0681      1362\n",
      "        B-scene     0.0104    0.0096    0.0100       209\n",
      "        I-scene     0.0068    0.0069    0.0069       722\n",
      "         B-book     0.0064    0.0065    0.0065       154\n",
      "         I-book     0.0267    0.0239    0.0252       877\n",
      "      B-company     0.0379    0.0397    0.0388       378\n",
      "      I-company     0.0655    0.0631    0.0643      1315\n",
      "     B-position     0.0395    0.0370    0.0382       433\n",
      "     I-position     0.0530    0.0482    0.0505       768\n",
      "   B-government     0.0421    0.0445    0.0433       247\n",
      "   I-government     0.0480    0.0515    0.0497      1068\n",
      "        B-movie     0.0352    0.0331    0.0341       151\n",
      "        I-movie     0.0463    0.0437    0.0450       892\n",
      "      avg/total     0.4494    0.5984    0.5133     50260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': {'Precision': 0.7085, 'Recall': 0.6796, 'F1': 0.6937, 'support': 465},\n",
       " 'address': {'Precision': 0.4297,\n",
       "  'Recall': 0.4424,\n",
       "  'F1': 0.4359,\n",
       "  'support': 373},\n",
       " 'organization': {'Precision': 0.6548,\n",
       "  'Recall': 0.7493,\n",
       "  'F1': 0.6989,\n",
       "  'support': 367},\n",
       " 'game': {'Precision': 0.7021, 'Recall': 0.8068, 'F1': 0.7508, 'support': 295},\n",
       " 'scene': {'Precision': 0.4922,\n",
       "  'Recall': 0.4545,\n",
       "  'F1': 0.4726,\n",
       "  'support': 209},\n",
       " 'book': {'Precision': 0.641, 'Recall': 0.6494, 'F1': 0.6452, 'support': 154},\n",
       " 'company': {'Precision': 0.6136,\n",
       "  'Recall': 0.6429,\n",
       "  'F1': 0.6279,\n",
       "  'support': 378},\n",
       " 'position': {'Precision': 0.721,\n",
       "  'Recall': 0.6744,\n",
       "  'F1': 0.6969,\n",
       "  'support': 433},\n",
       " 'government': {'Precision': 0.6552,\n",
       "  'Recall': 0.6923,\n",
       "  'F1': 0.6732,\n",
       "  'support': 247},\n",
       " 'movie': {'Precision': 0.6972,\n",
       "  'Recall': 0.6556,\n",
       "  'F1': 0.6758,\n",
       "  'support': 151},\n",
       " 'Total': {'Precision': 0.6346,\n",
       "  'Recall': 0.6491,\n",
       "  'F1': 0.6418,\n",
       "  'support': 3072}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型评估\r\n",
    "import score\r\n",
    "import importlib\r\n",
    "importlib.reload(score)\r\n",
    "from score import SingleClassificationScore\r\n",
    "\r\n",
    "\r\n",
    "def evaluate_detail_char_entity(model,test_loader):\r\n",
    "    # 定义统计评估指标的类\r\n",
    "    metric = SeqEntityScore(id2tag)\r\n",
    "    char_metric =  SingleClassificationScore(id2tag)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    with paddle.no_grad():\r\n",
    "        for step, batch in enumerate(test_loader):\r\n",
    "            # 获取数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            \r\n",
    "            # 前向计算，得出发射分数\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "\r\n",
    "            # 根据发射分数，利用CRF进行解码\r\n",
    "            scores, pred_paths = model.viterbi_decoder(features, batch_lens)\r\n",
    "\r\n",
    "            # 将这些预测的标签序列进行id2tag，即转换为相应的标签\r\n",
    "            pred_paths = [[id2tag[int(tag_id)] for tag_id in tag_seq] for tag_seq in pred_paths]\r\n",
    "            \r\n",
    "            # 根据文本序列的真实长度，对真实标签序列进行截断\r\n",
    "            batch_tags = batch_tags.numpy().tolist()\r\n",
    "            real_paths = [tag_seq[:tag_len] for tag_seq, tag_len in zip(batch_tags, batch_lens)]\r\n",
    "\r\n",
    "            # 更新统计指标相关数据\r\n",
    "            # print([y for x in pred_paths for y in x ])\r\n",
    "            assert len(pred_paths) == len(real_paths)\r\n",
    "            metric.update(pred_paths=pred_paths, real_paths=real_paths)\r\n",
    "            char_metric.update(pred_labels=[y for x in pred_paths for y in x ], real_labels=[y for x in real_paths for y in x ])\r\n",
    "\r\n",
    "    # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metric.get_result()\r\n",
    "    metric.format_print(result)\r\n",
    "    metric.report_scores(result)\r\n",
    "\r\n",
    "    result_2 = char_metric.get_result()\r\n",
    "    char_metric.report_scores(result_2)\r\n",
    "\r\n",
    "    return result \r\n",
    "\r\n",
    "evaluate_detail_char_entity(RERUNNER__model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 去除CRF层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Precision: 0.5004 - Recall: 0.6374 - F1: 0.5606\n",
      "                 precision    recall  f1-score   support\n",
      "           name     0.5957    0.6495    0.6214       465\n",
      "        address     0.2548    0.4236    0.3182       373\n",
      "   organization     0.6247    0.7439    0.6791       367\n",
      "           game     0.6676    0.7898    0.7236       295\n",
      "          scene     0.3186    0.4498    0.3730       209\n",
      "           book     0.4692    0.6429    0.5425       154\n",
      "        company     0.3552    0.6296    0.4542       378\n",
      "       position     0.7132    0.6721    0.6920       433\n",
      "     government     0.6429    0.6923    0.6667       247\n",
      "          movie     0.6600    0.6556    0.6578       151\n",
      "      avg/total     0.5004    0.6374    0.5606      3072\n",
      "                 precision    recall  f1-score   support\n",
      "         B-name     0.0316    0.0344    0.0329       465\n",
      "         I-name     0.0240    0.0852    0.0374      1021\n",
      "              O     0.5565    0.6421    0.5962     36747\n",
      "      B-address     0.0081    0.0134    0.0101       373\n",
      "      I-address     0.0341    0.0767    0.0472      1329\n",
      " B-organization     0.0183    0.0218    0.0199       367\n",
      " I-organization     0.0197    0.0258    0.0223      1087\n",
      "         B-game     0.0487    0.0576    0.0528       295\n",
      "         I-game     0.0626    0.0793    0.0699      1362\n",
      "        B-scene     0.0102    0.0144    0.0119       209\n",
      "        I-scene     0.0109    0.0249    0.0151       722\n",
      "         B-book     0.0047    0.0065    0.0055       154\n",
      "         I-book     0.0116    0.0376    0.0177       877\n",
      "      B-company     0.0254    0.0450    0.0324       378\n",
      "      I-company     0.0413    0.1027    0.0589      1315\n",
      "     B-position     0.0392    0.0370    0.0380       433\n",
      "     I-position     0.0442    0.0534    0.0484       768\n",
      "   B-government     0.0414    0.0445    0.0429       247\n",
      "   I-government     0.0445    0.0515    0.0477      1068\n",
      "        B-movie     0.0333    0.0331    0.0332       151\n",
      "        I-movie     0.0427    0.0437    0.0432       892\n",
      "      avg/total     0.3637    0.4843    0.4154     50260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': {'Precision': 0.5957, 'Recall': 0.6495, 'F1': 0.6214, 'support': 465},\n",
       " 'address': {'Precision': 0.2548,\n",
       "  'Recall': 0.4236,\n",
       "  'F1': 0.3182,\n",
       "  'support': 373},\n",
       " 'organization': {'Precision': 0.6247,\n",
       "  'Recall': 0.7439,\n",
       "  'F1': 0.6791,\n",
       "  'support': 367},\n",
       " 'game': {'Precision': 0.6676, 'Recall': 0.7898, 'F1': 0.7236, 'support': 295},\n",
       " 'scene': {'Precision': 0.3186, 'Recall': 0.4498, 'F1': 0.373, 'support': 209},\n",
       " 'book': {'Precision': 0.4692, 'Recall': 0.6429, 'F1': 0.5425, 'support': 154},\n",
       " 'company': {'Precision': 0.3552,\n",
       "  'Recall': 0.6296,\n",
       "  'F1': 0.4542,\n",
       "  'support': 378},\n",
       " 'position': {'Precision': 0.7132,\n",
       "  'Recall': 0.6721,\n",
       "  'F1': 0.692,\n",
       "  'support': 433},\n",
       " 'government': {'Precision': 0.6429,\n",
       "  'Recall': 0.6923,\n",
       "  'F1': 0.6667,\n",
       "  'support': 247},\n",
       " 'movie': {'Precision': 0.66, 'Recall': 0.6556, 'F1': 0.6578, 'support': 151},\n",
       " 'Total': {'Precision': 0.5004,\n",
       "  'Recall': 0.6374,\n",
       "  'F1': 0.5606,\n",
       "  'support': 3072}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型评估\r\n",
    "import score\r\n",
    "import importlib\r\n",
    "importlib.reload(score)\r\n",
    "from score import SingleClassificationScore\r\n",
    "\r\n",
    "\r\n",
    "def evaluate_detail_char_entity_no_CRF(model,test_loader):\r\n",
    "    # 定义统计评估指标的类\r\n",
    "    metric = SeqEntityScore(id2tag)\r\n",
    "    char_metric =  SingleClassificationScore(id2tag)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    with paddle.no_grad():\r\n",
    "        for step, batch in enumerate(test_loader):\r\n",
    "            # 获取数据\r\n",
    "            batch_ids, batch_tags, batch_mask, batch_lens = batch\r\n",
    "            \r\n",
    "            # 前向计算，得出发射分数\r\n",
    "            features, loss = model.forward_loss(batch_ids, batch_mask, batch_lens, batch_tags)\r\n",
    "\r\n",
    "            # 根据发射分数，利用CRF进行解码\r\n",
    "            # scores, pred_paths = model.viterbi_decoder(features, batch_lens)\r\n",
    "            pred_paths = paddle.argmax(features, axis= -1)\r\n",
    "\r\n",
    "            # 将这些预测的标签序列进行id2tag，即转换为相应的标签\r\n",
    "            pred_paths = [[id2tag[int(tag_id)] for tag_id in tag_seq] for tag_seq in pred_paths]\r\n",
    "            \r\n",
    "            # 根据文本序列的真实长度，对真实标签序列进行截断\r\n",
    "            batch_tags = batch_tags.numpy().tolist()\r\n",
    "            real_paths = [tag_seq[:tag_len] for tag_seq, tag_len in zip(batch_tags, batch_lens)]\r\n",
    "\r\n",
    "            # 更新统计指标相关数据\r\n",
    "            # print([y for x in pred_paths for y in x ])\r\n",
    "            assert len(pred_paths) == len(real_paths)\r\n",
    "            metric.update(pred_paths=pred_paths, real_paths=real_paths)\r\n",
    "            char_metric.update(pred_labels=[y for x in pred_paths for y in x ], real_labels=[y for x in real_paths for y in x ])\r\n",
    "\r\n",
    "    # 根据metric统计的数据，计算最终的准确率，召回率，F1值\r\n",
    "    result = metric.get_result()\r\n",
    "    metric.format_print(result)\r\n",
    "    metric.report_scores(result)\r\n",
    "\r\n",
    "    result_2 = char_metric.get_result()\r\n",
    "    char_metric.report_scores(result_2)\r\n",
    "\r\n",
    "    return result \r\n",
    "\r\n",
    "evaluate_detail_char_entity_no_CRF(RERUNNER__model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RERUNNER__model = load()\r\n",
    "transitionArray = RERUNNER__model.crf.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Tensor(shape=[33, 33], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
       "       [[-0.08526525, -0.03793711,  0.02859537, ...,  0.24255468,\n",
       "          0.28833938, -0.25289056],\n",
       "        [ 0.03477779, -0.29694635,  0.13645241, ..., -0.05103235,\n",
       "          0.24255556, -0.01399392],\n",
       "        [ 0.03728428, -0.25427681,  0.13968380, ...,  0.26362556,\n",
       "          0.16084141, -0.20982440],\n",
       "        ...,\n",
       "        [ 0.23981276,  0.19680473,  0.27714926, ...,  0.19684142,\n",
       "          0.19561511,  0.09494364],\n",
       "        [-0.10466477,  0.16904950,  0.21952766, ..., -0.17706488,\n",
       "         -0.11380799, -0.17421100],\n",
       "        [-0.27031475, -0.07000922,  0.22302569, ..., -0.15513097,\n",
       "          0.30032444, -0.01853615]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitionArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[33], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [-0.27031475, -0.07000922,  0.22302569, -0.11706991, -0.25752637,\n",
      "        -0.02069395, -0.11952141, -0.20108281, -0.28450519, -0.12538649,\n",
      "         0.24515440, -0.25085741, -0.09736122,  0.03068645,  0.14794850,\n",
      "         0.23071198,  0.14207439, -0.30041534,  0.13280988,  0.11484459,\n",
      "         0.17873847,  0.30134013, -0.17815727,  0.17397724,  0.08293276,\n",
      "         0.07850233, -0.28681079, -0.15948965, -0.24004097, -0.23333408,\n",
      "        -0.15513097,  0.30032444, -0.01853615])\n"
     ]
    }
   ],
   "source": [
    "print(transitionArray[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transition [-1]为start，[-2]为stop\r\n",
    "# 假设x[i,j]代表j->i，那么可以赋 transition[-1,:] = -1000,代表没有转移到start的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-address',\n",
       " 2: 'B-book',\n",
       " 3: 'B-company',\n",
       " 4: 'B-game',\n",
       " 5: 'B-government',\n",
       " 6: 'B-movie',\n",
       " 7: 'B-name',\n",
       " 8: 'B-organization',\n",
       " 9: 'B-position',\n",
       " 10: 'B-scene',\n",
       " 11: 'I-address',\n",
       " 12: 'I-book',\n",
       " 13: 'I-company',\n",
       " 14: 'I-game',\n",
       " 15: 'I-government',\n",
       " 16: 'I-movie',\n",
       " 17: 'I-name',\n",
       " 18: 'I-organization',\n",
       " 19: 'I-position',\n",
       " 20: 'I-scene',\n",
       " 21: 'S-address',\n",
       " 22: 'S-book',\n",
       " 23: 'S-company',\n",
       " 24: 'S-game',\n",
       " 25: 'S-government',\n",
       " 26: 'S-movie',\n",
       " 27: 'S-name',\n",
       " 28: 'S-organization',\n",
       " 29: 'S-position',\n",
       " 30: 'S-scene'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[33, 33], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[-0.08526525, -0.03793711,  0.02859537, ...,  0.24255468,\n",
      "          0.28833938, -0.25289056],\n",
      "        [ 0.03477779, -0.29694635,  0.13645241, ..., -0.05103235,\n",
      "          0.24255556, -0.01399392],\n",
      "        [ 0.03728428, -0.25427681,  0.13968380, ...,  0.26362556,\n",
      "          0.16084141, -0.20982440],\n",
      "        ...,\n",
      "        [ 0.23981276,  0.19680473,  0.27714926, ...,  0.19684142,\n",
      "          0.19561511,  0.09494364],\n",
      "        [-0.10466477,  0.16904950,  0.21952766, ..., -0.17706488,\n",
      "         -0.11380799, -0.17421100],\n",
      "        [-0.27031475, -0.07000922,  0.22302569, ..., -0.15513097,\n",
      "          0.30032444, -0.01853615]])\n",
      "Tensor(shape=[33, 33], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[-85.26525116 , -37.93711090 ,  28.59536743 , ...,\n",
      "          242.55467224,  288.33938599, -252.89056396],\n",
      "        [ 34.77779388 , -296.94635010,  136.45240784, ...,\n",
      "         -51.03234482 ,  242.55555725, -13.99392033 ],\n",
      "        [ 37.28427887 , -254.27680969,  139.68379211, ...,\n",
      "          263.62554932,  160.84140015, -209.82440186],\n",
      "        ...,\n",
      "        [ 239.81275940,  196.80473328,  277.14926147, ...,\n",
      "          196.84141541,  195.61511230,  94.94364166 ],\n",
      "        [-104.66476440,  169.04949951,  219.52766418, ...,\n",
      "         -177.06488037, -113.80799103, -174.21099854],\n",
      "        [-270.31475830, -70.00922394 ,  223.02569580, ...,\n",
      "         -155.13096619,  300.32443237, -18.53614616 ]])\n"
     ]
    }
   ],
   "source": [
    "# 更新参数并测试\r\n",
    "def load():\r\n",
    "    # load\r\n",
    "    RERUNNER_layer_state_dict = paddle.load(\"ReRunNer_model.pdparams\")\r\n",
    "    RERUNNER_opt_state_dict = paddle.load(\"ReRunNer_model.optparams\")\r\n",
    "\r\n",
    "    # 实例化模型\r\n",
    "    RERUNNER__model = NERModel(vocab_size=vocab_size, embedding_size=embedding_size,\r\n",
    "                        hidden_size=hidden_size,label2id=tag2id, n_layers=n_layers, drop_p=dropout_rate)\r\n",
    "\r\n",
    "    # 指定优化器\r\n",
    "    optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.99,\r\n",
    "                                    parameters=RERUNNER__model.parameters())\r\n",
    "    \r\n",
    "    transitions = RERUNNER_layer_state_dict['viterbi_decoder.transitions'].detach()\r\n",
    "\r\n",
    "    ###\r\n",
    "    print(transitions)\r\n",
    "    # transitions[-1,:] = -100\r\n",
    "    # transitions[-1,:] = -10000 #转移到statr(-1)的概率为0\r\n",
    "    # transitions[:,-2] = -10000 #从stop(-2)转移出去的概率为0\r\n",
    "\r\n",
    "    # transitions[12:20,1 ] = -10000\r\n",
    "    transitions = transitions * 1000\r\n",
    "\r\n",
    "    RERUNNER_layer_state_dict['viterbi_decoder.transitions'] = transitions\r\n",
    "    print(RERUNNER_layer_state_dict['viterbi_decoder.transitions'])\r\n",
    "    ###\r\n",
    "\r\n",
    "    RERUNNER__model.set_state_dict(RERUNNER_layer_state_dict)\r\n",
    "    optimizer.set_state_dict(RERUNNER_opt_state_dict)\r\n",
    "    return RERUNNER__model\r\n",
    "RERUNNER__model = load()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-18 22:05:57,115] [ WARNING] - Compatibility Warning: The params of LinearChainCrfLoss.forward has been modified. The third param is `labels`, and the fourth is not necessary. Please update the usage.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "31",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_128/3680257025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_detail_char_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRERUNNER__model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_128/3613779463.py\u001b[0m in \u001b[0;36mevaluate_detail_char_entity\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# 将这些预测的标签序列进行id2tag，即转换为相应的标签\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mpred_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# 根据文本序列的真实长度，对真实标签序列进行截断\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_128/3613779463.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# 将这些预测的标签序列进行id2tag，即转换为相应的标签\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mpred_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# 根据文本序列的真实长度，对真实标签序列进行截断\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_128/3613779463.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# 将这些预测的标签序列进行id2tag，即转换为相应的标签\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mpred_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# 根据文本序列的真实长度，对真实标签序列进行截断\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 31"
     ]
    }
   ],
   "source": [
    "evaluate_detail_char_entity(RERUNNER__model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
